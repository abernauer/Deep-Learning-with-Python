{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_4Fundamentalsofmachinelearning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNdUW0s3gnpEdLG1UDYvHAO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abernauer/Deep-Learning-with-Python/blob/master/Chapter_4Fundamentalsofmachinelearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYV88HvgOB_7",
        "colab_type": "text"
      },
      "source": [
        "* Central problem of machine learning: overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de1w20dFOegH",
        "colab_type": "text"
      },
      "source": [
        "#4.1 Four branches of machine learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB5XyxSSPMCQ",
        "colab_type": "text"
      },
      "source": [
        "In the previous examples, we covered: binary classification, multiclass classification, and scalar regression. All three fall under *supervised learning*, where the goal is to learn the relationship between training inputs and training targets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTSXt0u0Vbk4",
        "colab_type": "text"
      },
      "source": [
        "* consider going back and using TeX for math expressions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsxUrCAuPuOk",
        "colab_type": "text"
      },
      "source": [
        "#4.1.1 Supervised learning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfDKY1GXP-Y1",
        "colab_type": "text"
      },
      "source": [
        "Supervised learning consists of learning to map input data to known targets(also called *annotations*), given a set of examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuZ9nC5YQdFR",
        "colab_type": "text"
      },
      "source": [
        "Although supervised learning mostly consists of classification and regression, there are more exotic variants as well:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDg3rhKIQsFG",
        "colab_type": "text"
      },
      "source": [
        "* Sequence generation -- Given a picture, predict a caption describing it. Sequence generation can sometimes be reformulated as a series of classification (such as repeatedly predicting a word or token in a sequence).\n",
        "* *Syntax tree prediction* -- Given a sentence , predict its decomposition into a syntax tree.\n",
        "* *Object detection*--Given a picture, draw a bounding box around certain objects inside the picture. This can also be expressed as a classification problem(given many candidate bounding boxes, classify the contents of each one) or as a joint classification and regression problem, where the bounding-box coordinates are predicted via vector regression.\n",
        "* *Image segmentation*--Given a picture, draw a pixel-level mask on a specific object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttgmCeIzSyTO",
        "colab_type": "text"
      },
      "source": [
        "#4.1.2 Unsupervised learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11XkP7d_UGE5",
        "colab_type": "text"
      },
      "source": [
        "Unsupervised learning is the bread and butter of data analytics, and it's often a necessary step in better understanding a dataset before attempting to solve a supervised-learning problem. *Dimensionality reduction* and *clustering* are well-known categories of unsupervised learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiBRtMkuU6OW",
        "colab_type": "text"
      },
      "source": [
        "#4.1.3 Self-supervised learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sQcP_DNVLk3",
        "colab_type": "text"
      },
      "source": [
        "Self-supervised learning is supervised learning without human-annotated labels--you can think of it as supervised learning without any humans in the loop. Labels are still involved, but they're generated from the input data, typically using a heuristic algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KoF4LBEXc74",
        "colab_type": "text"
      },
      "source": [
        "For instance, *autoencoders* are a well-known instance of self-supervised learning, where the generated targets are the input, unmodified. In the same way, trying to predict the next frame in a video, given past frames, or the next word in a text, given previous words, are instances of self-supervised learning. Note that the distinction between supervised, self-supervised, and unsupervised learning can be blurry sometimes-- these categories are more of a continuum withouth solid borders. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0Wf5bilYmSt",
        "colab_type": "text"
      },
      "source": [
        "#4.1.4 Reinforcement learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqf03I1PYtlz",
        "colab_type": "text"
      },
      "source": [
        "In reinforcement learning, an *agent* receives information about its environment and learns to choose actions that will maximize some reward. For instance, a neural network that \"looks\" at a video game screen and outputs game actions in order to maximize its score can be trained via reinforcement learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFAE3ZaWfTN0",
        "colab_type": "text"
      },
      "source": [
        "#Classification and regression glossary\n",
        "\n",
        "* Sample or input--One data point that goes into your model.\n",
        "* Prediction or output-- What comes out of your model.\n",
        "* Target -- The truth, What your model should ideally have predicted, according to an external source of data.\n",
        "* Prediction error or loss value-- A measure of the distance between your model's prediction and the target.\n",
        "* Classes-- A set of possible labels to choose from in a classification problem. For example, when classifying cat and dog pictures, \"dog\" and \"cat\" are the two classes.\n",
        "* Ground-truth or annotations-- All targets for a dataset, typically collected by humans.\n",
        "* Binary classification -- A classification task where each input sample should be categorized into two exclusive categories.\n",
        "* Multiclass classification-- A classification task where each input sample should be categorized into more than two categories: for instance, classifying handwritten digits.\n",
        "* Multilabel classification-- A classification task where each input sample can be assigned multiple labels. For instances, a given image may contain both a cat and a dog and should be annotated both with the \"cat\" label and the \"dog\" label. The number of labels per image is usually variable.\n",
        "* Vector regression -- A task where the target is a set of continous values: for example, a continuous vector.\n",
        "* Mini-batch or batch -- A small set of samples(typically between 8 and 128) that are processed simultaneously by the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlLMjBwwiqmt",
        "colab_type": "text"
      },
      "source": [
        "#4.2 Evaluating machine-learning models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS5vIsDYlMm5",
        "colab_type": "text"
      },
      "source": [
        "* First key problem in machine learning avoid *overfitting*. Or performing better on the training data and stalling on the test data after a few iterations or epochs.\n",
        "\n",
        "* Second key problem *generalize* performance on to never before seen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR_5aCjGmCAM",
        "colab_type": "text"
      },
      "source": [
        "#4.2.1 Training, validation, and test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzGf3y7hmLK5",
        "colab_type": "text"
      },
      "source": [
        "1. Split the data into three sets: training, validation, and test.\n",
        "2. Train on your training data and evaluate you model on the validation data.\n",
        "3. Test the model on the test data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL8__xbVoncW",
        "colab_type": "text"
      },
      "source": [
        "The validation set allows us to tune the configuration of our model: choosing the number of layers or size of the layers( *hyperparameters*). Tuning the hyperparameters is a form of *learning*: a search problem for a good configuration in some parameter space. This can result in *overfitting on the validation set*.\n",
        "\n",
        "*Information leaks* may take place if you optimize your hyperparameters too much on the validation set as you will expose information to the model that will tune it specifically for performance on the validation set. Not for the test set which we would like to perform well on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17KgonHir5i0",
        "colab_type": "text"
      },
      "source": [
        "# Simple Hold-out validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg23zRJasD--",
        "colab_type": "text"
      },
      "source": [
        "Set apart some fraction of your data as your test set. Train on the remaining data, and evauate on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bib7w7luvgYU",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        " import numpy as np\n",
        " from keras import models\n",
        "\n",
        " num_validation_samples = 10000\n",
        "\n",
        " np.random.shuffle(data)\n",
        "\n",
        " validation_data = data[:num_validation_samples]\n",
        " data = data[num_validation_samples:]\n",
        "\n",
        " training_data = data[:]\n",
        "\n",
        " model = get_model()\n",
        " model.train(training_data)\n",
        " validation_score = model.evaluate(validation_data)\n",
        "\n",
        " # At this point you can tune your model.\n",
        " # retrain it, evaluate it, tune it again.\n",
        "\n",
        " model = get_model()\n",
        " model.train(np.concatenate([training_data,\n",
        "                             validation_data]))\n",
        " test_score = model.evaluate(test_data)\n",
        "\n",
        " ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2A_XnnkBVrd",
        "colab_type": "text"
      },
      "source": [
        "This is the simplest evaluation protocol, and it suffers from one flaw: if little data is available, then your validattion and test sets may contain too few samples to be statistically representative of the data at hand. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2NhVNpJChyK",
        "colab_type": "text"
      },
      "source": [
        "# K-FOLD Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpZpT-mRCsCe",
        "colab_type": "text"
      },
      "source": [
        "With this approach, you split your data into K partitions of equal size. For each partition i, train a model on the remaining K-1 partitions, and evaluate it on partition i. Your final score is then the averages of the K scores obtained. This method is helpful when the performance of your model shows significant variance based on your training test split. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSM4cTIDCrHd",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "k = 4\n",
        "num_validation_samples = len(data) // k\n",
        "\n",
        "np.random.shuffle(data)\n",
        "\n",
        "validation_scores = []\n",
        "for fold in range(k):\n",
        "    validation_data = data[num_validation_samples * fold:\n",
        "    num_validation_samples * (fold + 1)]\n",
        "    training_data = data[:num_validation_samples * fold] + \n",
        "    data[num_validation_samples * (fold + 1):]\n",
        "    model = get_model()\n",
        "    model.train(training_data)\n",
        "    validation_score = model.evaluate(validation_data)\n",
        "    validation_scores.append(validation_score)\n",
        "validation_score = np.average(validation_scores)\n",
        "\n",
        "model = get_model()\n",
        "model.train(data)\n",
        "test_score = model.evaluate(test_data)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEsNhRz-F7Nl",
        "colab_type": "text"
      },
      "source": [
        "#Iterated K-FOLD VALIDATION WITH SHUFFLING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeNBPyJtGDUZ",
        "colab_type": "text"
      },
      "source": [
        "It consists of applying K-fold validation multiple times, shuffing the data every time before splitting it K ways. The final score is the average of the scores obtained at each run of K-fold validation. Note that you end up training and evaluating P X K models(where P is the number of iterations you use), which can be very expensive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueJnhVVkG0zn",
        "colab_type": "text"
      },
      "source": [
        "#4.2.2 Things to keep in mind"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GN4q3jxHF2l",
        "colab_type": "text"
      },
      "source": [
        "* *Data representiveness*-- You want both your taining set and test set to be representative of the data at hand. For instance, if you're trying to classify images of digits, and you're starting from an array of samples where the samples are ordered by their class, takinge the first 80% of the array as your training set and the remaining 20% as your test set will result in your training set containing only classes 0-7, whereas your test set contatins only classes 8-9. This seems like a ridiculous mistake, but it's surprisingly common. For this reason, you usually should *randomly shuffle* your data before splitting it into training and test sets.\n",
        "\n",
        "* *The arrow of time*-- If you're trying to predict the future given the past, you should not randomly shuffle your data before splitting it, because doing so will create a *temporal leak*: your model will effectively be trained on data from the future. In such situations you should always make sure all data in your test set *posterior* to the data in the training set.\n",
        "\n",
        "* *Redundancy in your data*-- If some data points in your data appear twice, then shuffling the data and splitting it into a training set and validation set will result in redundacy between the training and validation sets. In effect, you'll be testing on part of your training data, which is the worst thing you can do! Make sure your training set and validation set are disjoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryMAEGudJ8nC",
        "colab_type": "text"
      },
      "source": [
        "#4.3 Data preprocessing, feature engineering, and feature learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZwEZz_waz0L",
        "colab_type": "text"
      },
      "source": [
        "In addition to model evaluation, an important question we must must tackle before we dive deeper into model development is the following: how do you prepare the input data and targets before feeding them into a neural network?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKeIjrP8jAZ3",
        "colab_type": "text"
      },
      "source": [
        "#4.3.1 Data preprocessing for neural networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QuVgTNnjTCe",
        "colab_type": "text"
      },
      "source": [
        "#Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn2j_MSSjXQH",
        "colab_type": "text"
      },
      "source": [
        "All inputs and targets in a neural network must be tensors of floating-point. Whatever data you need to process you must first turn into tensors, a step called *data vectorization*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6JYJyU7orRB",
        "colab_type": "text"
      },
      "source": [
        "#Value normalization "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-WM6k44owcx",
        "colab_type": "text"
      },
      "source": [
        "In general, it isn't safe to feed into a neural network data that takes a relatively large values or data that is heterogeneous. Doing so can trigger large gradient updates that will prevent the network from converging. \n",
        "\n",
        "* *Take small values*-- Typically, most values should be in the 0-1 range.\n",
        "* Be *homogenous*-- That is, all features should take values in roughly the same range.\n",
        "\n",
        "* Normalize each feature independently to have a mean of 0.\n",
        "\n",
        "* Normalize each feature independently to have a standard deviation of 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ckgtiMjqDw_",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "x -= x.mean(axis=0)\n",
        "x /= x.std(axis=0)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOBzMhE7qNN0",
        "colab_type": "text"
      },
      "source": [
        "#Handling Missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDB6xGo1qSvX",
        "colab_type": "text"
      },
      "source": [
        "In general, with neural networks, it's safe to input missing values as 0, with the condition that 0 isn't already a meaningful value. The network will learn from exposure to the data that the value 0 means *missing data* and will start ignoring the value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwiCtVjyrK-v",
        "colab_type": "text"
      },
      "source": [
        "Note that if you're expecting missing values in the test data, but the network was trained on data without any missing values in the test data, the network won't have learned to ignore missing values!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXz3f-Irr95G",
        "colab_type": "text"
      },
      "source": [
        "#4.3.2 Feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WNa_-ussEoY",
        "colab_type": "text"
      },
      "source": [
        "*Feature engineering* is the process of using your own knowledge about the data and about the machine-learning algorithm at hand to make the algorithm work better by applying hardcoded transformations to the data before it goes into the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pon04DPIvK_Y",
        "colab_type": "text"
      },
      "source": [
        "The essence of feature engineering is: making a problem easier by expressing it in a simpler way. Requiring some domain knowledge about the problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apVSxMN2vZTl",
        "colab_type": "text"
      },
      "source": [
        "* Good features still allow you to solve problems more elegantly while using fewer resources. For instance, it would be ridiculous to solve the problem of reading a clock face using a convolutional neural network.\n",
        "\n",
        "* Good features let you solve a problem with far less data. The ability of deep-learning models to learn features on their own relies on having lots of training data available; if you have only a few samples, then the information value in their features becomes critical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KORWbE8dwG6E",
        "colab_type": "text"
      },
      "source": [
        "#4.4 Overfitting and underfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GjntAZKx0dY",
        "colab_type": "text"
      },
      "source": [
        "*Optimization* refers to the process of adjusting a model to get the best performance possible on the training data (the *learning* in *machine learning*), whereas *generalization* refers to how well the trained model performs on data it has never seen before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrZ5qGzbzube",
        "colab_type": "text"
      },
      "source": [
        "At the beginning of training, optimization and generalization are correlated: the lower the loss on the training data, the lower the loss on test data. While this is happening, your model is said to be *underfit:* there is still progress to be made; the network hasn't yet modeled all revelant patterns in the training data. As training continues, generalization and validation metrics stall while the network learns patterns specific to the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxisCeCy1abQ",
        "colab_type": "text"
      },
      "source": [
        "To prevent a model from learning misleading or irrelevant patterns found in the training data, *the best solution is to get more training data*. A model trained on more data will naturally generalize better. If a network can only afford to memorize a small number of patterns, the optimization process will force it to focus on the most prominent patterns, which have a better chance of generalizing well.\n",
        "\n",
        "The process of fighting overfitting this way is called *reguralization*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUB9deuN3EJv",
        "colab_type": "text"
      },
      "source": [
        "#4.4.1 Reducing the network's size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxKcwzvn3Ky9",
        "colab_type": "text"
      },
      "source": [
        "The simplest way to prevent overfitting is to reduce the size of the model: the number of learnable parameters in the model( which is determined by the number of layers and the number of units per laye). In deep learning, the number of learnable parameters in a model is often referred to as the model's *capacity*. Intuitively, a model with more parameters has more *memorization capacity* and therefore can easily learn a perfect dictionary-like mapping between training samples and their targets--a mapping between training samples and their targets--a mapping without any generalization power."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6srQs45ESnY",
        "colab_type": "text"
      },
      "source": [
        "On the other hand, if the network has limited memorization resources, it won't be ablle to learn this mapping as easily; thus, in order to minimize its loss, it will have to resort to learning compressed representations that have predictive power regarding the targets--precisely the type of representations we're interested in. At the same time, keep in mind that you should use models that have enough parameters that they don't underfit: your model shouldn't be starved for memorization resources. There is a compromise to be found between *too much capacity* and *not enough capacity*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eS_JiU-JXcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD5Pt7nxKTYi",
        "colab_type": "text"
      },
      "source": [
        "Smaller network for movie-review classification network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSHVXmauKeh3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "d4964c3b-04da-42db-885f-ddb2056e12c1"
      },
      "source": [
        "model = model.Sequential()\n",
        "model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(4, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9af401095e60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'Sequential'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mPTFLp8LObx",
        "colab_type": "text"
      },
      "source": [
        "The smaller network starts overfitting later than the reference network, and its performance degrads more slowly once it starts overfitting."
      ]
    }
  ]
}