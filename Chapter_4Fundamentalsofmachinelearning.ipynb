{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_4Fundamentalsofmachinelearning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMWoJla6yvboIIPOBmSwqj9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abernauer/Deep-Learning-with-Python/blob/master/Chapter_4Fundamentalsofmachinelearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYV88HvgOB_7",
        "colab_type": "text"
      },
      "source": [
        "* Central problem of machine learning: overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de1w20dFOegH",
        "colab_type": "text"
      },
      "source": [
        "#4.1 Four branches of machine learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB5XyxSSPMCQ",
        "colab_type": "text"
      },
      "source": [
        "In the previous examples, we covered: binary classification, multiclass classification, and scalar regression. All three fall under *supervised learning*, where the goal is to learn the relationship between training inputs and training targets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsxUrCAuPuOk",
        "colab_type": "text"
      },
      "source": [
        "#4.1.1 Supervised learning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfDKY1GXP-Y1",
        "colab_type": "text"
      },
      "source": [
        "Supervised learning consists of learning to map input data to known targets(also called *annotations*), given a set of examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuZ9nC5YQdFR",
        "colab_type": "text"
      },
      "source": [
        "Although supervised learning mostly consists of classification and regression, there are more exotic variants as well:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDg3rhKIQsFG",
        "colab_type": "text"
      },
      "source": [
        "* Sequence generation -- Given a picture, predict a caption describing it. Sequence generation can sometimes be reformulated as a series of classification (such as repeatedly predicting a word or token in a sequence).\n",
        "* *Syntax tree prediction* -- Given a sentence , predict its decomposition into a syntax tree.\n",
        "* *Object detection*--Given a picture, draw a bounding box around certain objects inside the picture. This can also be expressed as a classification problem(given many candidate bounding boxes, classify the contents of each one) or as a joint classification and regression problem, where the bounding-box coordinates are predicted via vector regression.\n",
        "* *Image segmentation*--Given a picture, draw a pixel-level mask on a specific object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttgmCeIzSyTO",
        "colab_type": "text"
      },
      "source": [
        "#4.1.2 Unsupervised learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11XkP7d_UGE5",
        "colab_type": "text"
      },
      "source": [
        "Unsupervised learning is the bread and butter of data analytics, and it's often a necessary step in better understanding a dataset before attempting to solve a supervised-learning problem. *Dimensionality reduction* and *clustering* are well-known categories of unsupervised learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiBRtMkuU6OW",
        "colab_type": "text"
      },
      "source": [
        "#4.1.3 Self-supervised learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sQcP_DNVLk3",
        "colab_type": "text"
      },
      "source": [
        "Self-supervised learning is supervised learning without human-annotated labels--you can think of it as supervised learning without any humans in the loop. Labels are still involved, but they're generated from the input data, typically using a heuristic algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KoF4LBEXc74",
        "colab_type": "text"
      },
      "source": [
        "For instance, *autoencoders* are a well-known instance of self-supervised learning, where the generated targets are the input, unmodified. In the same way, trying to predict the next frame in a video, given past frames, or the next word in a text, given previous words, are instances of self-supervised learning. Note that the distinction between supervised, self-supervised, and unsupervised learning can be blurry sometimes-- these categories are more of a continuum withouth solid borders. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0Wf5bilYmSt",
        "colab_type": "text"
      },
      "source": [
        "#4.1.4 Reinforcement learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqf03I1PYtlz",
        "colab_type": "text"
      },
      "source": [
        "In reinforcement learning, an *agent* receives information about its environment and learns to choose actions that will maximize some reward. For instance, a neural network that \"looks\" at a video game screen and outputs game actions in order to maximize its score can be trained via reinforcement learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFAE3ZaWfTN0",
        "colab_type": "text"
      },
      "source": [
        "#Classification and regression glossary\n",
        "\n",
        "* Sample or input--One data point that goes into your model.\n",
        "* Prediction or output-- What comes out of your model.\n",
        "* Target -- The truth, What your model should ideally have predicted, according to an external source of data.\n",
        "* Prediction error or loss value-- A measure of the distance between your model's prediction and the target.\n",
        "* Classes-- A set of possible labels to choose from in a classification problem. For example, when classifying cat and dog pictures, \"dog\" and \"cat\" are the two classes.\n",
        "* Ground-truth or annotations-- All targets for a dataset, typically collected by humans.\n",
        "* Binary classification -- A classification task where each input sample should be categorized into two exclusive categories.\n",
        "* Multiclass classification-- A classification task where each input sample should be categorized into more than two categories: for instance, classifying handwritten digits.\n",
        "* Multilabel classification-- A classification task where each input sample can be assigned multiple labels. For instances, a given image may contain both a cat and a dog and should be annotated both with the \"cat\" label and the \"dog\" label. The number of labels per image is usually variable.\n",
        "* Vector regression -- A task where the target is a set of continous values: for example, a continuous vector.\n",
        "* Mini-batch or batch -- A small set of samples(typically between 8 and 128) that are processed simultaneously by the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlLMjBwwiqmt",
        "colab_type": "text"
      },
      "source": [
        "#4.2 Evaluating machine-learning models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS5vIsDYlMm5",
        "colab_type": "text"
      },
      "source": [
        "* First key problem in machine learning avoid *overfitting*. Or performing better on the training data and stalling on the test data after a few iterations or epochs.\n",
        "\n",
        "* Second key problem *generalize* performance on to never before seen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR_5aCjGmCAM",
        "colab_type": "text"
      },
      "source": [
        "#4.2.1 Training, validation, and test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzGf3y7hmLK5",
        "colab_type": "text"
      },
      "source": [
        "1. Split the data into three sets: training, validation, and test.\n",
        "2. Train on your training data and evaluate you model on the validation data.\n",
        "3. Test the model on the test data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL8__xbVoncW",
        "colab_type": "text"
      },
      "source": [
        "The validation set allows us to tune the configuration of our model: choosing the number of layers or size of the layers( *hyperparameters*). Tuning the hyperparameters is a form of *learning*: a search problem for a good configuration in some parameter space. This can result in *overfitting on the validation set*.\n",
        "\n",
        "*Information leaks* may take place if you optimize your hyperparameters too much on the validation set as you will expose information to the model that will tune it specifically for performance on the validation set. Not for the test set which we would like to perform well on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17KgonHir5i0",
        "colab_type": "text"
      },
      "source": [
        "# Simple Hold-out validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg23zRJasD--",
        "colab_type": "text"
      },
      "source": [
        "Set apart som"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bib7w7luvgYU",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        " import numpy as np\n",
        " from keras import models\n",
        "\n",
        " num_validation_samples = 10000\n",
        "\n",
        " np.random.shuffle(data)\n",
        "\n",
        " validation_data = data[:num_validation_samples]\n",
        " data = data[num_validation_samples:]\n",
        "\n",
        " training_data = data[:]\n",
        "\n",
        " model = get_model()\n",
        " model.train(training_data)\n",
        " validation_score = model.evaluate(validation_data)\n",
        "\n",
        " # At this point you can tune your model.\n",
        " # retrain it, evaluate it, tune it again.\n",
        "\n",
        " model = get_model()\n",
        " model.train(np.concatenate([training_data,\n",
        "                             validation_data]))\n",
        " test_score = model.evaluate(test_data)\n",
        "\n",
        " ```"
      ]
    }
  ]
}