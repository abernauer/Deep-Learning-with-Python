{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter5Deeplearningforcomputervision.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO1JuYA7/kjZMVOam2y5eyK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abernauer/Deep-Learning-with-Python/blob/master/Chapter5Deeplearningforcomputervision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDNebN4mNGZ3",
        "colab_type": "text"
      },
      "source": [
        "#Chapter 5: Deep learning for computer vision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx6xaNdzNRys",
        "colab_type": "text"
      },
      "source": [
        "#5.1 Introduction to convnets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AllhzeSONerQ",
        "colab_type": "text"
      },
      "source": [
        "We're about to dive into the theory of what convnets are and why they have been so successful at computer vision tasks. First will revisit the MNIST digit example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqpZehMKM-DO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers \n",
        "from keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D(2, 2))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1pITugW2wP8",
        "colab_type": "text"
      },
      "source": [
        "A convnet takes as input tensors of shape( image_height, image_width, image_channels) (not including the batch dimension). In this case, we'll configure the convnet to process inputs of size (28, 28, 1), which is the format of MNIST images. We did this by passing the argument input_shape=(28, 28, 1) to the first layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INCWgJQc38FB",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the architecture so far:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyfTakHJ4Afv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "e620e964-15fc-4130-fde8-2b24a14c91a6"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 3, 3, 64)          36928     \n",
            "=================================================================\n",
            "Total params: 55,744\n",
            "Trainable params: 55,744\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpRCZNTX4YIW",
        "colab_type": "text"
      },
      "source": [
        "The output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink as you go deeper in the network. The number of channels is controlled by the first argument passed to the Conv2D layers (32 or 64).\n",
        "\n",
        "The next step is to feed the last output tensor (of shape (3, 3, 64)) into a densely connected classifier network like those you're already familiar with: a stack of Dense layers. These classifiers process vectors, which are 1D, whereas the current output is a 3D tensor. First we have to flatten the 3D outputs to 1D, and then add a few Dense layers on top."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7VL9tcO5tVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(layers.Flatten())       \n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfXuou98BpW0",
        "colab_type": "text"
      },
      "source": [
        "We'll do 10-way classification, using a final layer with 10 outputs and a sofmax activitaion.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-lXpI1rBp0M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "413823cc-f0ce-46bd-8ec0-54a3b676189e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 64)                36928     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 93,322\n",
            "Trainable params: 93,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73qDopLsDvch",
        "colab_type": "text"
      },
      "source": [
        "The (3, 3, 64) outputs are flattened into vectors of shape (576, ) before going through two Dense layers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zmxkREjEnDe",
        "colab_type": "text"
      },
      "source": [
        "Let's train the convnet on the MNIST digits.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND42LdyVEnim",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "c662b0fa-4ca0-46e6-d574-5c5eb1e6ae1e"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "(train_images, train_labels),  (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 49s 816us/step - loss: 0.1768 - accuracy: 0.9441\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 49s 813us/step - loss: 0.0459 - accuracy: 0.9856\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 48s 799us/step - loss: 0.0320 - accuracy: 0.9901\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 48s 793us/step - loss: 0.0239 - accuracy: 0.9921\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 47s 791us/step - loss: 0.0181 - accuracy: 0.9945\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fd4820dc240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22Wp-IKzGPTI",
        "colab_type": "text"
      },
      "source": [
        "Let's evaluate the model on the test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAcTVip7HakT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "75698fbb-4aef-4cc6-fb80-9a21de253d40"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 3s 286us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01szJWPKHklk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4af9c0d2-50b1-49f2-e25d-e9262843cd6b"
      },
      "source": [
        "test_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9916999936103821"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQw28ba0HpXk",
        "colab_type": "text"
      },
      "source": [
        "The basic convnet has a test accuracy of 99.2% which reduced the error rate in comparison the densely connected network from chapter 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60zA-9_IISMK",
        "colab_type": "text"
      },
      "source": [
        "#5.1.1 The convolution operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8EeiPD2JRg-",
        "colab_type": "text"
      },
      "source": [
        "The fundamental difference between a densely connected layer and a convolution layer is this: Dense layers learn global pattern in their input feature space, whereas convolution layers learn local patterns: in the case of images, patterns found in small 2D windows of the inputs. In the previous example, these windows were all 3 x 3. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znlSq-3VJ2Rq",
        "colab_type": "text"
      },
      "source": [
        "This key characteristic gives convnets two interesting properties:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcJQFj6jKFj2",
        "colab_type": "text"
      },
      "source": [
        "* *The patterns they learn are translation invariant*. After learning a certain pattern in the lower-rigth corner of a picture, a convnet can recognize it anywhere: for example, in the upper-left corner. A densely connected network would have to learn pattern annew if it appeared at a new location. This makes convnets data efficient when processing images (because *the visual world is fundamentally translation invariant*): they need fewer training samples to learn representations that have generalization power.\n",
        "\n",
        "* *They can learn spatial hierarchies of patterns*. A first convolution layer will learn small local patterns such as edges, a second convolution layer will learn larger patterns made of the features of the first layers, and so on. This allows convnets to efficiently learn increasingly complex and abstract visual concepts (because *the visual world is fundamentally spatially hierarchical*.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVKvLuPJM-j_",
        "colab_type": "text"
      },
      "source": [
        "Convolutions operate over 3D tensors, called *feature maps*, with two spatial axes( *height* and *width*) as well as a *depth* axis (also called the *channels* axis). For an RGB image, the dimension of the depth axis is 3, because the image has three color channels: red, green, and blue. The convolution operation extracts patches from its input feature map and applies the same transformation to all of these patches, producing an *output feature map*. This output feature map is still a 3D tensor: it has a width and a height. Its depth can be arbitrary, because the output depth is a parameter of the layer, and the different channels in that depth axis no longer stand for specific colors as in RGB input; rather, they stand for *filters*. Filters encode specific apsects of the input: at a high level, a single filter could encode the concept \"presence of a face in the input,\" for instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR7PeSgkEiVY",
        "colab_type": "text"
      },
      "source": [
        "In the MNIST example, the first convolution layer takes a feature map of size (28, 28, 1) and outputs a feature map of size (26, 26, 32): it computes 32 filters over its input. Each of these 32 output channels contains a 26 X 26 grid of values, which is a *response map* of the filter over the input, indicating the response of that filter pattern at different locations in the input. That is what the term *feature map* means: every dimension in the depth axis is a feature (or filter), and the 2D tensor output[:, :, n] is the 2D spatial *map* of the response of this filter over the input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shMB2n6mHiEG",
        "colab_type": "text"
      },
      "source": [
        "Convolutions are defined by two key parameters:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xohUGUcQHoAC",
        "colab_type": "text"
      },
      "source": [
        "* *Size of the patches extracted from the inputs* -- These are typically 3 x 3 or 5 x 5. In the example, they were 3 x 3, which is a common choice.\n",
        "* *Depth of the output feature map* -- The number of filters computed by the convolution. The example started it out with a depth of 32 and ended with a depth of 64."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OyLcPajI3QY",
        "colab_type": "text"
      },
      "source": [
        "In Keras Conv2D layers, these parameters are the first arguments passed to the layer: Conv2D(output_depth, (window_height, window_width))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUezV6xcJy1k",
        "colab_type": "text"
      },
      "source": [
        "A convolution works by *sliding* these windows of size 3 x 3 or 5 x 5 over the 3D input feature map, stopping at every possible location, and extracting the 3D patch of surrounding features. Each such 3D patch is then transformed (via a tensor product with the same learned weigth matrix, called the *convolution kernel*) into a 1D vector of shape (output_depth,). All of these vectors are then spatially reassembled into a 3D output map of shape (height, width, output_depth). Every spatial location in the output feature map corresponds to the same location in the input feature map. For instance, with 3 x 3 windows, the vector output[i, j, :] comes from the 3D patch input[i-1:i+1, j-1:j+1, :]. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z59TPYLSabRV",
        "colab_type": "text"
      },
      "source": [
        "Note that the output width and height may differ from the input width and height. They may differ for two reasons:\n",
        "* Border effects, which can be countered by padding the input feature map\n",
        "* The use of *strides*, which I'll define in a second."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bXpZrmBbEKJ",
        "colab_type": "text"
      },
      "source": [
        "# UNDERSTANDING BORDER EFFECTS AND PADDING "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUBTg3m1bMgY",
        "colab_type": "text"
      },
      "source": [
        "Consider a 5 x 5 feature map (25 tiles total). There are only 9 tiles around which you can center a 3 x 3 window, forming a 3 x 3 grid. Hence, the output feature map will be 3 x 3. It shrinks a little; by exactly two tiles alongside each dimension, in this case. You can see this border effect in action in the earlier example: you start with 28 x 28, which shrinks to 26 x 26 after the first convolution layer.\n",
        "\n",
        "If you want to get an output feature map with the same spatial dimensions as the input, you can use *padding*. Padding consists of adding an appropriate number of rows and columns on each side of the input feature map so as to make it possible to fit center convolution windows around every input tile. For a 3 x 3 window, you add one column on the right, one column on the left, on row at the top, and one row at the bottom. For a 5 x 5 window, you add two rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK5LNFsYq0OX",
        "colab_type": "text"
      },
      "source": [
        "In Conv2D layers, padding is configurable via the padding argument, whic takes two values: \"valid\", which means no padding (only valid window locations will be used); and \"same\", which means \"pad in such a way as to have an output with the same width and height as the input.\" The padding argument defaults to \"valid\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irHi6keMt9Yq",
        "colab_type": "text"
      },
      "source": [
        "#UNDERSTANDING CONVOLUTION STRIDES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZYN6pb0BtZ7",
        "colab_type": "text"
      },
      "source": [
        "The other factor that can influenc output size is the notion of *strides*. The description of convolutio so far has assumed that the center tiles of the convolution windows are all contiguous. But the distance, between two succesive windows is a parameter fo the convolution, called its *stride*, which defaults to 1. It's possible to have *strided convolutions*: convolutions with a stride higher than 1. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxdIMU7tC_r5",
        "colab_type": "text"
      },
      "source": [
        "Using stride 2 means the witdh and height of the feature map are downsampled by a factor of 2 (in addition to any changes induced by border effects). Strided convolutions are rarely used in practice, although they can come in handy for some types of models; it's good to be familiar with the concept."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dplSSZ1El0Q",
        "colab_type": "text"
      },
      "source": [
        "To downsample feature maps, instead of strides, we tend to use the *max-pooling* operation, which you saw in action in the first convnet example. Let's look at it in more depth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE-Ph_IoE83c",
        "colab_type": "text"
      },
      "source": [
        "#The max-pooling operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFglV_NSGW6u",
        "colab_type": "text"
      },
      "source": [
        "In the convnet example, you may have noticed that size of the feature maps is halved after every MaxPooling2D layer. For instance, before the first MaxPooling2D layers, the feature map is 26 x 26, but the max-pooling operation halves it to 13 x 13. That's the role of max pooling: to aggressively downsample feature maps, much like strided convolutions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH92HebjGW2B",
        "colab_type": "text"
      },
      "source": [
        "Max pooling consists of extracting windows from the input feature maps and outputting the max value of each channel. It's conceptually similar to convolution, except that instead of transforming local patches via a learned linear transformation(the convolution kernel), they're transformed via a hardcoded max tensor operation. A big difference from convolution is that max pooling is usually done with 2 x 2 windows and stride 2, in order to downsample the feature maps by a factor of 2. On the other hand, convolution is typically done with 3 x 3 windows and no stride (stride 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yavjsT7MObZl",
        "colab_type": "text"
      },
      "source": [
        "Why downsample feature maps this way?  Why not remove the max-pooling layers and keep fairly large feature maps all the way up? Let's look at this option. The convolutional base of the model would then look like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUvcaWk_RNBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model_no_max_pool = models.Sequential()\n",
        "model_no_max_pool.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "input_shape=(28, 28, 1)))\n",
        "model_no_max_pool.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model_no_max_pool.add(layers.Conv2D(64, (3, 3), activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVvXX-M3TSFA",
        "colab_type": "text"
      },
      "source": [
        "Here's a summary of the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvmsOTbyTRk8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "9953efd9-7ba4-4899-c257-fe8283896905"
      },
      "source": [
        "model_no_max_pool.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 22, 22, 64)        36928     \n",
            "=================================================================\n",
            "Total params: 55,744\n",
            "Trainable params: 55,744\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_tLf7aGUq8h",
        "colab_type": "text"
      },
      "source": [
        "What's wrong with this setup? Two things:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5rQLE-nUv-Q",
        "colab_type": "text"
      },
      "source": [
        "* It isn't conducive to learning a spatial hierarchy of features. The 3 x 3 windows in the third layer will only contain information coming from 7 x 7 windows in the initial input. The high-level pattern learned by the convnet will still be very small with regard to the intial input, which may not be enough to learn to classify digits. We need the features from the last convolution layer to contain information about the totality of the input.\n",
        "\n",
        "* The final feature map has 22 x 22 x 64 = 30,976 total coefficients per sample. This is huge. If you were to flatten it to stick a Dense layer of size 512 on top, that layer would have 15.8 million parameters. This is far too large for such a small model and would result in intense overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKYwYjy5YUQu",
        "colab_type": "text"
      },
      "source": [
        "In short, the reason to use downnsampling is to reduce the number of feature-map coefficients to process, as well as to induce spatial-filter hierarchies by making successive convolution layers look at increasingly large windows (in terms of the fraction of the original input they cover). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpYmMEJuY1Po",
        "colab_type": "text"
      },
      "source": [
        "Note that max pooling isn't the only way you can acheive such downsampling. As you already know, you can also use strides in the prior convolution layer. And you can use average pooling instead of max pooling, where each local input patch is transformed by taking the average pooling instead of max pooling, where each local input patch is transformed by taking the average value of each channel over the patch, rather than the max. But max pooling tends to work better than these alternative solutions. In a nutshell, the reason is that features tend to encode the spatial presence of some pattern or concept over the different tiles of the feature map, and it's more informative to look at the *maximal presence* of different features than at their *average presence*. So the most reasonable subsampling strategy is to first produce dense maps of features and then look at the maximal activation of the features over small patches, rather than looking at sparser windows of the inputs or averaging input patches, which could cause you to miss or dilute feature-presence information. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZUil59wVK1J",
        "colab_type": "text"
      },
      "source": [
        "#5.2 Training a convent from scratch on a small dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RYXeA0NWY0a",
        "colab_type": "text"
      },
      "source": [
        "Having to train an image-classification model using very little data is a common situation, which you'll likely ecnounter in practice if you ever do computer vision in a professional context. A \"few\" samples can mean anywhere from a few hundred to a few tens of thousands of images. As a practical example, we'll focus on classifying images as dogs or cats, in a dataset containing 4,000 pictures of cats and dogs. We'll use 2,000 pictures for training-- 1,000 for validation, and 1,000 for testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLcI8BUfYmsE",
        "colab_type": "text"
      },
      "source": [
        "In this section, we'll review one basic strategy to tackle this problem: training a new model from scratch using what little data you have. You'll start by natively training a small convnet on the 2,000 training samples, without any regularization, to set a baseline for what can be achieved. This will get you to a classification accuracry of 71%. At that point, the main issue will be overfitting. Then we'll introduce *data augmentation*, a powerful technique for mitigating overfitting in computer vision. By using data augmentation, you'll improve the network to reach an accuracy of 82%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMQe8KygiuLh",
        "colab_type": "text"
      },
      "source": [
        "In the next section, we'll review two more essential techniques for applying deep learning to small datasets: *faeture extraction with a pretrained network* (improving accuracy to 96% from 90%) and *fine-tuning a pretrained network* (getting to an accuracy of 97%) These strategies will form the workflow or toolbox for image classification on small datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqSSxwsT-Zd5",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0CKmGRWDM7Y",
        "colab_type": "text"
      },
      "source": [
        "Listing 5.4 Copying images to training, validation, and test directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSJuLZdaCg21",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "23b2931e-1710-415b-9e2c-cad00a0c8f89"
      },
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive', force_remount = True)\n",
        "  is_google_colab = True\n",
        "  print(\"In Google CoLab\")\n",
        "except:\n",
        "  is_google_colab = False\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "In Google CoLab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWJnDVYlA6d7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "f5dea14f-c714-4788-ca88-2ac7880ea332"
      },
      "source": [
        "!find /content/drive/My\\ Drive/Colab\\ Notebooks/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/\n",
            "/content/drive/My Drive/Colab Notebooks/Copy of Chapter2:building blocks of neural nets.ipynb\n",
            "/content/drive/My Drive/Colab Notebooks/Chapter2:building blocks of neural nets.ipynb\n",
            "/content/drive/My Drive/Colab Notebooks/ListComphrensions.ipynb\n",
            "/content/drive/My Drive/Colab Notebooks/Chapter3_Getting_started_with_neural_networks.ipynb\n",
            "/content/drive/My Drive/Colab Notebooks/Chapter_4Fundamentalsofmachinelearning.ipynb\n",
            "/content/drive/My Drive/Colab Notebooks/dogs-vs-cats.zip\n",
            "/content/drive/My Drive/Colab Notebooks/Chapter5Deeplearningforcomputervision.ipynb\n",
            "/content/drive/My Drive/Colab Notebooks/Python for Data Scientists & Engineers.ipynb\n",
            "/content/drive/My Drive/Colab Notebooks/Copy of Python for Data Scientists & Engineers.ipynb\n",
            "/content/drive/My Drive/Colab Notebooks/sampleSubmission.csv\n",
            "/content/drive/My Drive/Colab Notebooks/test1.zip\n",
            "/content/drive/My Drive/Colab Notebooks/train.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrlnbASrprsZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "7286da95-7ad8-47d5-a5e8-1c971ef6980e"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cats_and_dogs_small  sample_data\t   test1      train\n",
            "drive\t\t     sampleSubmission.csv  test1.zip  train.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXoWyp-kp6A2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e5525165-2ed6-412a-b858-47c9cf5b0173"
      },
      "source": [
        "!mv train/*.jpg ./drive/My\\ Drive/Colab\\ Notebooks\\"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPodc1VMrgvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv test1/*.jpg ./drive/My\\ Drive/Colab\\ Notebooks\\"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0OSZvD2CriY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bc51f464-cbd2-43ce-e785-1e89582f240c"
      },
      "source": [
        "import os, shutil\n",
        "# from earlier find ./drive/My\\ Drive/Colab\\ Notebooks/dogs-vs-cats.zip\n",
        "# /content/drive/My Drive/\n",
        "print(os.getcwd())\n",
        "# current wd is gdrive\n",
        "\n",
        "original_dataset_dir = '/content/drive/My Drive/Colab Notebooks'\n",
        "\n",
        "base_dir = '/content/drive/My Drive/Colab Notebooks/cats_and_dogs_small'\n",
        "os.mkdir(base_dir)\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.mkdir(test_dir)\n",
        "\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "os.mkdir(train_cats_dir)\n",
        "\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "os.mkdir(train_dogs_dir)\n",
        "\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "os.mkdir(validation_cats_dir)\n",
        "\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "os.mkdir(validation_dogs_dir)\n",
        "\n",
        "test_cats_dir = os.path.join(test_dir, 'cats')\n",
        "os.mkdir(test_cats_dir)\n",
        "\n",
        "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
        "os.mkdir(test_dogs_dir)\n",
        "\n",
        "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir, fname)\n",
        "    dst = os.path.join(train_cats_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "\n",
        "fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir, fname)\n",
        "    dst = os.path.join(validation_cats_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "\n",
        "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir, fname)\n",
        "    dst = os.path.join(test_cats_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "\n",
        "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir, fname)\n",
        "    dst = os.path.join(train_dogs_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "\n",
        "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir, fname)\n",
        "    dst = os.path.join(validation_dogs_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "\n",
        "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir, fname)\n",
        "    dst = os.path.join(test_dogs_dir, fname)\n",
        "    shutil.copyfile(src, dst)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnqqDZKiwTSe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "049d1b80-1d78-49ab-a3fd-85c89955ec7c"
      },
      "source": [
        "print('total training cat images:', len(os.listdir(train_cats_dir)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total training cat images: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgvIuXclwzTe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5b62adc0-283f-4b4d-f0b9-3a20419b5cdf"
      },
      "source": [
        "print('total training dog images:', len(os.listdir(train_dogs_dir)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total training dog images: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnuW2QptxFiM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d7ff25e5-0c71-4e1f-e2a1-deb0ff62d94f"
      },
      "source": [
        "print('total validation cat images:', len(os.listdir(validation_cats_dir)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total training dog images: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6zKqxcPxaRP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a8bf405f-7fac-4d81-a038-5cafa69dd11b"
      },
      "source": [
        "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total validation dog images: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sat9iOfxw_r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6031abf7-09dc-4f03-d6bf-d406fe858409"
      },
      "source": [
        "print('total test cat images:', len(os.listdir(test_cats_dir)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total test cat images: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Nxf6tgRyJ5Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a2657407-c07f-4e50-cf0f-42a47524f22d"
      },
      "source": [
        "print('total test dog images:', len(os.listdir(test_dogs_dir)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total test dog images: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AT6CKPxbtmD",
        "colab_type": "text"
      },
      "source": [
        "After refactoring the code for google Colab, We have 2,000 training imgages, 1,000 validation images, and 1,000 test images. Each split contains the same number of samples from each class: this is a balanced binary-classification problem, so we can use classification accuracy as a measure of success."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqehLmkCbuMd",
        "colab_type": "text"
      },
      "source": [
        "#5.2.3 Building your network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uepk3Txy75_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D(2, 2))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D(2, 2))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D(2, 2))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D(2, 2))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqpx5mwv-ayR",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the model summary and how the feature map changes with each layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynPEKyxr-ly9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "5145eba1-306a-475e-dadc-2011528d98f0"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               3211776   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 3,453,121\n",
            "Trainable params: 3,453,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjbvCERK--OG",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byRfz7aD--iR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import optimizers\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTaFve4aQROM",
        "colab_type": "text"
      },
      "source": [
        "#5.2.4 Data preprocessing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF1R22LFQYt9",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWUd44-dQZWO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "3826a4dd-cae3-4cc4-d0fb-5f1d0edc1f9f"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftR2PzLVXojn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "fa3c7305-0ed3-4db1-8a2c-4c7bfd706208"
      },
      "source": [
        "for data_batch, labels_batch in train_generator:\n",
        "    print('data batch shape:', data_batch.shape)\n",
        "    print('labels batch shape:', labels_batch.shape)\n",
        "    break"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data batch shape: (20, 150, 150, 3)\n",
            "labels batch shape: (20,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KONKMB8YULg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d35e0f3d-7372-46b4-808b-ea6f40bb139c"
      },
      "source": [
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=30,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "100/100 [==============================] - 118s 1s/step - loss: 0.6895 - acc: 0.5385 - val_loss: 0.7088 - val_acc: 0.5390\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - 115s 1s/step - loss: 0.6596 - acc: 0.6130 - val_loss: 0.5722 - val_acc: 0.6200\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - 116s 1s/step - loss: 0.6130 - acc: 0.6615 - val_loss: 0.6123 - val_acc: 0.6290\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - 118s 1s/step - loss: 0.5670 - acc: 0.7065 - val_loss: 0.7807 - val_acc: 0.5970\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - 118s 1s/step - loss: 0.5356 - acc: 0.7245 - val_loss: 0.5517 - val_acc: 0.6810\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - 118s 1s/step - loss: 0.5075 - acc: 0.7420 - val_loss: 0.5914 - val_acc: 0.6840\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - 118s 1s/step - loss: 0.4747 - acc: 0.7680 - val_loss: 0.5648 - val_acc: 0.7180\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - 118s 1s/step - loss: 0.4427 - acc: 0.7900 - val_loss: 0.5798 - val_acc: 0.7020\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - 120s 1s/step - loss: 0.4154 - acc: 0.8120 - val_loss: 0.4598 - val_acc: 0.7130\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - 120s 1s/step - loss: 0.4019 - acc: 0.8205 - val_loss: 0.5815 - val_acc: 0.7180\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - 120s 1s/step - loss: 0.3690 - acc: 0.8335 - val_loss: 0.6429 - val_acc: 0.7370\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - 121s 1s/step - loss: 0.3449 - acc: 0.8550 - val_loss: 0.6051 - val_acc: 0.7110\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - 120s 1s/step - loss: 0.3233 - acc: 0.8570 - val_loss: 0.8898 - val_acc: 0.7360\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - 120s 1s/step - loss: 0.3015 - acc: 0.8715 - val_loss: 0.6191 - val_acc: 0.7400\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - 120s 1s/step - loss: 0.2825 - acc: 0.8740 - val_loss: 0.5856 - val_acc: 0.7500\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - 119s 1s/step - loss: 0.2663 - acc: 0.8870 - val_loss: 0.4720 - val_acc: 0.7460\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - 117s 1s/step - loss: 0.2325 - acc: 0.9110 - val_loss: 0.5627 - val_acc: 0.7210\n",
            "Epoch 18/30\n",
            "100/100 [==============================] - 117s 1s/step - loss: 0.2112 - acc: 0.9175 - val_loss: 0.8019 - val_acc: 0.7370\n",
            "Epoch 19/30\n",
            "100/100 [==============================] - 117s 1s/step - loss: 0.1906 - acc: 0.9265 - val_loss: 1.4007 - val_acc: 0.7340\n",
            "Epoch 20/30\n",
            "100/100 [==============================] - 120s 1s/step - loss: 0.1807 - acc: 0.9290 - val_loss: 0.3879 - val_acc: 0.7400\n",
            "Epoch 21/30\n",
            "100/100 [==============================] - 119s 1s/step - loss: 0.1537 - acc: 0.9430 - val_loss: 0.9144 - val_acc: 0.6970\n",
            "Epoch 22/30\n",
            "100/100 [==============================] - 121s 1s/step - loss: 0.1320 - acc: 0.9560 - val_loss: 0.5320 - val_acc: 0.7390\n",
            "Epoch 23/30\n",
            "100/100 [==============================] - 119s 1s/step - loss: 0.1231 - acc: 0.9605 - val_loss: 0.6720 - val_acc: 0.7470\n",
            "Epoch 24/30\n",
            "100/100 [==============================] - 117s 1s/step - loss: 0.1001 - acc: 0.9665 - val_loss: 0.9079 - val_acc: 0.7450\n",
            "Epoch 25/30\n",
            "100/100 [==============================] - 119s 1s/step - loss: 0.0881 - acc: 0.9760 - val_loss: 0.4268 - val_acc: 0.7080\n",
            "Epoch 26/30\n",
            "100/100 [==============================] - 119s 1s/step - loss: 0.0785 - acc: 0.9760 - val_loss: 0.5583 - val_acc: 0.7420\n",
            "Epoch 27/30\n",
            "100/100 [==============================] - 118s 1s/step - loss: 0.0679 - acc: 0.9800 - val_loss: 1.2002 - val_acc: 0.7240\n",
            "Epoch 28/30\n",
            "100/100 [==============================] - 118s 1s/step - loss: 0.0586 - acc: 0.9860 - val_loss: 0.8602 - val_acc: 0.7260\n",
            "Epoch 29/30\n",
            "100/100 [==============================] - 118s 1s/step - loss: 0.0470 - acc: 0.9895 - val_loss: 0.7077 - val_acc: 0.7450\n",
            "Epoch 30/30\n",
            "100/100 [==============================] - 119s 1s/step - loss: 0.0413 - acc: 0.9875 - val_loss: 1.5321 - val_acc: 0.7410\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W62Q9jKvdKbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('cats_and_dogs_small_1.h5')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmdvEDBcmiX8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "c2c4553e-a91f-4479-e13b-0b56e8550fb2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}