{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter5Deeplearningforcomputervision.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOBM+d6InT2xCnZy8QAs8s7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abernauer/Deep-Learning-with-Python/blob/master/Chapter5Deeplearningforcomputervision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDNebN4mNGZ3",
        "colab_type": "text"
      },
      "source": [
        "#Chapter 5: Deep learning for computer vision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx6xaNdzNRys",
        "colab_type": "text"
      },
      "source": [
        "#5.1 Introduction to convnets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AllhzeSONerQ",
        "colab_type": "text"
      },
      "source": [
        "We're about to dive into the theory of what convnets are and why they have been so successful at computer vision tasks. First will revisit the MNIST digit example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqpZehMKM-DO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers \n",
        "from keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D(2, 2))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1pITugW2wP8",
        "colab_type": "text"
      },
      "source": [
        "A convnet takes as input tensors of shape( image_height, image_width, image_channels) (not including the batch dimension). In this case, we'll configure the convnet to process inputs of size (28, 28, 1), which is the format of MNIST images. We did this by passing the argument input_shape=(28, 28, 1) to the first layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INCWgJQc38FB",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the architecture so far:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyfTakHJ4Afv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "e620e964-15fc-4130-fde8-2b24a14c91a6"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 3, 3, 64)          36928     \n",
            "=================================================================\n",
            "Total params: 55,744\n",
            "Trainable params: 55,744\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpRCZNTX4YIW",
        "colab_type": "text"
      },
      "source": [
        "The output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink as you go deeper in the network. The number of channels is controlled by the first argument passed to the Conv2D layers (32 or 64).\n",
        "\n",
        "The next step is to feed the last output tensor (of shape (3, 3, 64)) into a densely connected classifier network like those you're already familiar with: a stack of Dense layers. These classifiers process vectors, which are 1D, whereas the current output is a 3D tensor. First we have to flatten the 3D outputs to 1D, and then add a few Dense layers on top."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7VL9tcO5tVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(layers.Flatten())       \n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfXuou98BpW0",
        "colab_type": "text"
      },
      "source": [
        "We'll do 10-way classification, using a final layer with 10 outputs and a sofmax activitaion.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-lXpI1rBp0M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "413823cc-f0ce-46bd-8ec0-54a3b676189e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 64)                36928     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 93,322\n",
            "Trainable params: 93,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73qDopLsDvch",
        "colab_type": "text"
      },
      "source": [
        "The (3, 3, 64) outputs are flattened into vectors of shape (576, ) before going through two Dense layers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zmxkREjEnDe",
        "colab_type": "text"
      },
      "source": [
        "Let's train the convnet on the MNIST digits.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND42LdyVEnim",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "c662b0fa-4ca0-46e6-d574-5c5eb1e6ae1e"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "(train_images, train_labels),  (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 49s 816us/step - loss: 0.1768 - accuracy: 0.9441\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 49s 813us/step - loss: 0.0459 - accuracy: 0.9856\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 48s 799us/step - loss: 0.0320 - accuracy: 0.9901\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 48s 793us/step - loss: 0.0239 - accuracy: 0.9921\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 47s 791us/step - loss: 0.0181 - accuracy: 0.9945\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fd4820dc240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22Wp-IKzGPTI",
        "colab_type": "text"
      },
      "source": [
        "Let's evaluate the model on the test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAcTVip7HakT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "75698fbb-4aef-4cc6-fb80-9a21de253d40"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 3s 286us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01szJWPKHklk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4af9c0d2-50b1-49f2-e25d-e9262843cd6b"
      },
      "source": [
        "test_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9916999936103821"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQw28ba0HpXk",
        "colab_type": "text"
      },
      "source": [
        "The basic convnet has a test accuracy of 99.2% which reduced the error rate in comparison the densely connected network from chapter 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60zA-9_IISMK",
        "colab_type": "text"
      },
      "source": [
        "#5.1.1 The convolution operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8EeiPD2JRg-",
        "colab_type": "text"
      },
      "source": [
        "The fundamental difference between a densely connected layer and a convolution layer is this: Dense layers learn global pattern in their input feature space, whereas convolution layers learn local patterns: in the case of images, patterns found in small 2D windows of the inputs. In the previous example, these windows were all 3 x 3. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znlSq-3VJ2Rq",
        "colab_type": "text"
      },
      "source": [
        "This key characteristic gives convnets two interesting properties:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcJQFj6jKFj2",
        "colab_type": "text"
      },
      "source": [
        "* *The patterns they learn are translation invariant*. After learning a certain pattern in the lower-rigth corner of a picture, a convnet can recognize it anywhere: for example, in the upper-left corner. A densely connected network would have to learn pattern annew if it appeared at a new location. This makes convnets data efficient when processing images (because *the visual world is fundamentally translation invariant*): they need fewer training samples to learn representations that have generalization power.\n",
        "\n",
        "* *They can learn spatial hierarchies of patterns*. A first convolution layer will learn small local patterns such as edges, a second convolution layer will learn larger patterns made of the features of the first layers, and so on. This allows convnets to efficiently learn increasingly complex and abstract visual concepts (because *the visual world is fundamentally spatially hierarchical*.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVKvLuPJM-j_",
        "colab_type": "text"
      },
      "source": [
        "Convolutions operate over 3D tensors, called *feature maps*, with two spatial axes( *height* and *width*) as well as a *depth* axis (also called the *channels* axis). For an RGB image, the dimension of the depth axis is 3, because the image has three color channels: red, green, and blue. The convolution operation extracts patches from its input feature map and applies the same transformation to all of these patches, producing an *output feature map*. This output feature map is still a 3D tensor: it has a width and a height. Its depth can be arbitrary, because the output depth is a parameter of the layer, and the different channels in that depth axis no longer stand for specific colors as in RGB input; rather, they stand for *filters*. Filters encode specific apsects of the input: at a high level, a single filter could encode the concept \"presence of a face in the input,\" for instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR7PeSgkEiVY",
        "colab_type": "text"
      },
      "source": [
        "In the MNIST example, the first convolution layer takes a feature map of size (28, 28, 1) and outputs a feature map of size (26, 26, 32): it computes 32 filters over its input. Each of these 32 output channels contains a 26 X 26 grid of values, which is a *response map* of the filter over the input, indicating the response of that filter pattern at different locations in the input. That is what the term *feature map* means: every dimension in the depth axis is a feature (or filter), and the 2D tensor output[:, :, n] is the 2D spatial *map* of the response of this filter over the input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shMB2n6mHiEG",
        "colab_type": "text"
      },
      "source": [
        "Convolutions are defined by two key parameters:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xohUGUcQHoAC",
        "colab_type": "text"
      },
      "source": [
        "* *Size of the patches extracted from the inputs* -- These are typically 3 x 3 or 5 x 5. In the example, they were 3 x 3, which is a common choice.\n",
        "* *Depth of the output feature map* -- The number of filters computed by the convolution. The example started it out with a depth of 32 and ended with a depth of 64."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OyLcPajI3QY",
        "colab_type": "text"
      },
      "source": [
        "In Keras Conv2D layers, these parameters are the first arguments passed to the layer: Conv2D(output_depth, (window_height, window_width))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUezV6xcJy1k",
        "colab_type": "text"
      },
      "source": [
        "A convolution works by *sliding* these windows of size 3 x 3 or 5 x 5 over the 3D input feature map, stopping at every possible location, and extracting the 3D patch of surrounding features. Each such 3D patch is then transformed (via a tensor product with the same learned weigth matrix, called the *convolution kernel*) into a 1D vector of shape (output_depth,). All of these vectors are then spatially reassembled into a 3D output map of shape (height, width, output_depth). Every spatial location in the output feature map corresponds to the same location in the input feature map. For instance, with 3 x 3 windows, the vector output[i, j, :] comes from the 3D patch input[i-1:i+1, j-1:j+1, :]. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z59TPYLSabRV",
        "colab_type": "text"
      },
      "source": [
        "Note that the output width and height may differ from the input width and height. They may differ for two reasons:\n",
        "* Border effects, which can be countered by padding the input feature map\n",
        "* The use of *strides*, which I'll define in a second."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bXpZrmBbEKJ",
        "colab_type": "text"
      },
      "source": [
        "# UNDERSTANDING BORDER EFFECTS AND PADDING "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUBTg3m1bMgY",
        "colab_type": "text"
      },
      "source": [
        "Consider a 5 x 5 feature map (25 tiles total). There are only 9 tiles around which you can center a 3 x 3 window, forming a 3 x 3 grid. Hence, the output feature map will be 3 x 3. It shrinks a little; by exactly two tiles alongside each dimension, in this case. You can see this border effect in action in the earlier example: you start with 28 x 28, which shrinks to 26 x 26 after the first convolution layer.\n",
        "\n",
        "If you want to get an output feature map with the same spatial dimensions as the input, you can use *padding*. Padding consists of adding an appropriate number of rows and columns on each side of the input feature map so as to make it possible to fit center convolution windows around every input tile. For a 3 x 3 window, you add one column on the right, one column on the left, on row at the top, and one row at the bottom. For a 5 x 5 window, you add two rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK5LNFsYq0OX",
        "colab_type": "text"
      },
      "source": [
        "In Conv2D layers, padding is configurable via the padding argument, whic takes two values: \"valid\", which means no padding (only valid window locations will be used); and \"same\", which means \"pad in such a way as to have an output with the same width and height as the input.\" The padding argument defaults to \"valid\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irHi6keMt9Yq",
        "colab_type": "text"
      },
      "source": [
        "#UNDERSTANDING CONVOLUTION STRIDES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZYN6pb0BtZ7",
        "colab_type": "text"
      },
      "source": [
        "The other factor that can influenc output size is the notion of *strides*. The description of convolutio so far has assumed that the center tiles of the convolution windows are all contiguous. But the distance, between two succesive windows is a parameter fo the convolution, called its *stride*, which defaults to 1. It's possible to have *strided convolutions*: convolutions with a stride higher than 1. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxdIMU7tC_r5",
        "colab_type": "text"
      },
      "source": [
        "Using stride 2 means the witdh and height of the feature map are downsampled by a factor of 2 (in addition to any changes induced by border effects). Strided convolutions are rarely used in practice, although they can come in handy for some types of models; it's good to be familiar with the concept."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dplSSZ1El0Q",
        "colab_type": "text"
      },
      "source": [
        "To downsample feature maps, instead of strides, we tend to use the *max-pooling* operation, which you saw in action in the first convnet example. Let's look at it in more depth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE-Ph_IoE83c",
        "colab_type": "text"
      },
      "source": [
        "#The max-pooling operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFglV_NSGW6u",
        "colab_type": "text"
      },
      "source": [
        "In the convnet example, you may have noticed that size of the feature maps is halved after every MaxPooling2D layer. For instance, before the first MaxPooling2D layers, the feature map is 26 x 26, but the max-pooling operation halves it to 13 x 13. That's the role of max pooling: to aggressively downsample feature maps, much like strided convolutions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH92HebjGW2B",
        "colab_type": "text"
      },
      "source": [
        "Max pooling consists of extracting windows from the input feature maps and outputting the max value of each channel. It's conceptually similar to convolution, except that instead of transforming local patches via a learned linear transformation(the convolution kernel), they're transformed via a hardcoded max tensor operation. A big difference from convolution is that max pooling is usually done with 2 x 2 windows and stride 2, in order to downsample the feature maps by a factor of 2. On the other hand, convolution is typically done with 3 x 3 windows and no stride (stride 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yavjsT7MObZl",
        "colab_type": "text"
      },
      "source": [
        "Why downsample feature maps this way?  Why not remove the max-pooling layers and keep fairly large feature maps all the way up? Let's look at this option. The convolutional base of the model would then look like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUvcaWk_RNBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model_no_max_pool = models.Sequential()\n",
        "model_no_max_pool.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "input_shape=(28, 28, 1)))\n",
        "model_no_max_pool.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model_no_max_pool.add(layers.Conv2D(64, (3, 3), activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVvXX-M3TSFA",
        "colab_type": "text"
      },
      "source": [
        "Here's a summary of the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvmsOTbyTRk8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "9953efd9-7ba4-4899-c257-fe8283896905"
      },
      "source": [
        "model_no_max_pool.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 22, 22, 64)        36928     \n",
            "=================================================================\n",
            "Total params: 55,744\n",
            "Trainable params: 55,744\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_tLf7aGUq8h",
        "colab_type": "text"
      },
      "source": [
        "What's wrong with this setup? Two things:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5rQLE-nUv-Q",
        "colab_type": "text"
      },
      "source": [
        "* It isn't conducive to learning a spatial hierarchy of features. The 3 x 3 windows in the third layer will only contain information coming from 7 x 7 windows in the initial input. The high-level pattern learned by the convnet will still be very small with regard to the intial input, which may not be enough to learn to classify digits. We need the features from the last convolution layer to contain information about the totality of the input.\n",
        "\n",
        "* The final feature map has 22 x 22 x 64 = 30,976 total coefficients per sample. This is huge. If you were to flatten it to stick a Dense layer of size 512 on top, that layer would have 15.8 million parameters. This is far too large for such a small model and would result in intense overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKYwYjy5YUQu",
        "colab_type": "text"
      },
      "source": [
        "In short, the reason to use downnsampling is to reduce the number of feature-map coefficients to process, as well as to induce spatial-filter hierarchies by making successive convolution layers look at increasingly large windows (in terms of the fraction of the original input they cover). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpYmMEJuY1Po",
        "colab_type": "text"
      },
      "source": [
        "Note that max pooling isn't the only way you can acheive such downsampling. As you already know, you can also use strides in the prior convolution layer. And you can use average pooling instead of max pooling, where each local input patch is transformed by taking the average pooling instead of max pooling, where each local input patch is transformed by taking the average value of each channel over the patch, rather than the max. But max pooling tends to work better than these alternative solutions. In a nutshell, the reason is that features tend to encode the spatial presence of some pattern or concept over the different tiles of the feature map, and it's more informative to look at the *maximal presence* of different features than at their *average presence*. So the most reasonable subsampling strategy is to first produce dense maps of features and then look at the maximal activation of the features over small patches, rather than looking at sparser windows of the inputs or averaging input patches, which could cause you to miss or dilute feature-presence information. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZUil59wVK1J",
        "colab_type": "text"
      },
      "source": [
        "#5.2 Training a convent from scratch on a small dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RYXeA0NWY0a",
        "colab_type": "text"
      },
      "source": [
        "Having to train an image-classification model using very little data is a common situation, which you'll likely ecnounter in practice if you ever do computer vision in a professional context. A \"few\" samples can mean anywhere from a few hundred to a few tens of thousands of images. As a practical example, we'll focus on classifying images as dogs or cats, in a dataset containing 4,000 pictures of cats and dogs. We'll use 2,000 pictures for training-- 1,000 for validation, and 1,000 for testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLcI8BUfYmsE",
        "colab_type": "text"
      },
      "source": [
        "In this section, we'll review one basic strategy to tackle this problem: training a new model from scratch using what little data you have. You'll start by natively training a small convnet on the 2,000 training samples, without any regularization, to set a baseline for what can be achieved. This will get you to a classification accuracry of 71%. At that point, the main issue will be overfitting. Then we'll introduce *data augmentation*, a powerful technique for mitigating overfitting in computer vision. By using data augmentation, you'll improve the network to reach an accuracy of 82%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMQe8KygiuLh",
        "colab_type": "text"
      },
      "source": [
        "In the next section, we'll review two more essential techniques for applying deep learning to small datasets: *faeture extraction with a pretrained network* (improving accuracy to 96% from 90%) and *fine-tuning a pretrained network* (getting to an accuracy of 97%) These strategies will form the workflow or toolbox for image classification on small datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqSSxwsT-Zd5",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0CKmGRWDM7Y",
        "colab_type": "text"
      },
      "source": [
        "Listing 5.4 Copying images to training, validation, and test directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSJuLZdaCg21",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "533a38de-1f18-461c-f474-1ab2e51cb8a5"
      },
      "source": [
        "import zipfile\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqiIwVM9WVpV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8e8615bd-8ce1-4d43-ba8a-0da373e5e690"
      },
      "source": [
        "!find . *.zip\n",
        "!unzip ./drive/My\\ Drive/Colab\\ Notebooks/dogs-vs-cats.zip"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "./.config\n",
            "./.config/active_config\n",
            "./.config/configurations\n",
            "./.config/configurations/config_default\n",
            "./.config/.last_opt_in_prompt.yaml\n",
            "./.config/config_sentinel\n",
            "./.config/.last_survey_prompt.yaml\n",
            "./.config/logs\n",
            "./.config/logs/2020.07.10\n",
            "./.config/logs/2020.07.10/16.28.19.621895.log\n",
            "./.config/logs/2020.07.10/16.28.52.396365.log\n",
            "./.config/logs/2020.07.10/16.29.12.134415.log\n",
            "./.config/logs/2020.07.10/16.29.12.848063.log\n",
            "./.config/logs/2020.07.10/16.28.38.231204.log\n",
            "./.config/logs/2020.07.10/16.28.57.265459.log\n",
            "./.config/.last_update_check.json\n",
            "./.config/gce\n",
            "./.config/.metricsUUID\n",
            "./drive\n",
            "./drive/.shortcut-targets-by-id\n",
            "./drive/My Drive\n",
            "./drive/My Drive/INTE.gslides\n",
            "./drive/My Drive/Untitled document (26).gdoc\n",
            "./drive/My Drive/Untitled document (25).gdoc\n",
            "./drive/My Drive/ CORAL REEF BIOME.gdoc\n",
            "./drive/My Drive/Untitled document (24).gdoc\n",
            "./drive/My Drive/sources.gdoc\n",
            "./drive/My Drive/Untitled document (23).gdoc\n",
            "./drive/My Drive/Untitled document (22).gdoc\n",
            "./drive/My Drive/Untitled document (21).gdoc\n",
            "./drive/My Drive/brit lit notes.gdoc\n",
            "./drive/My Drive/Untitled document (20).gdoc\n",
            "./drive/My Drive/travel site\\.gdoc\n",
            "./drive/My Drive/The Rape of the Lock.gdoc\n",
            "./drive/My Drive/Untitled document (19).gdoc\n",
            "./drive/My Drive/Untitled document (18).gdoc\n",
            "./drive/My Drive/Biology Chapter 8 outline.gdoc\n",
            "./drive/My Drive/Untitled document (17).gdoc\n",
            "./drive/My Drive/Untitled document (16).gdoc\n",
            "./drive/My Drive/chapter 9.gdoc\n",
            "./drive/My Drive/Bio Diagrams PT1.jpg\n",
            "./drive/My Drive/Bio Diagrams.jpg\n",
            "./drive/My Drive/Bio Diagrams PT1 (1).jpg.gdoc\n",
            "./drive/My Drive/Bio Diagrams PT1.jpg.gdoc\n",
            "./drive/My Drive/Bio Diagrams.jpg.gdoc\n",
            "./drive/My Drive/bio chapter 8 part 2.gdoc\n",
            "./drive/My Drive/brady bunch.gdoc\n",
            "./drive/My Drive/hod paper.gdoc\n",
            "./drive/My Drive/Museum Visit .gdoc\n",
            "./drive/My Drive/Untitled document (15).gdoc\n",
            "./drive/My Drive/sonnets.gdoc\n",
            "./drive/My Drive/Play League of Legends.lnk\n",
            "./drive/My Drive/Untitled document (14).gdoc\n",
            "./drive/My Drive/Untitled document (13).gdoc\n",
            "./drive/My Drive/ap gov note cards.gdoc\n",
            "./drive/My Drive/essay questions.gdoc\n",
            "./drive/My Drive/Untitled document (12).gdoc\n",
            "./drive/My Drive/Untitled document (11).gdoc\n",
            "./drive/My Drive/orwell questions.gdoc\n",
            "./drive/My Drive/Cyber Bulling Outline.gdoc\n",
            "./drive/My Drive/Untitled document (10).gdoc\n",
            "./drive/My Drive/Section 4.docx\n",
            "./drive/My Drive/Untitled document (9).gdoc\n",
            "./drive/My Drive/leadership essay.gdoc\n",
            "./drive/My Drive/case notes.gdoc\n",
            "./drive/My Drive/Untitled document (8).gdoc\n",
            "./drive/My Drive/andrewcollege  copy.gsheet\n",
            "./drive/My Drive/IOUG.gdoc\n",
            "./drive/My Drive/Malcolm x essay.gdoc\n",
            "./drive/My Drive/epic notes.gdoc\n",
            "./drive/My Drive/desc writing notes.gdoc\n",
            "./drive/My Drive/iliad annotating.gdoc\n",
            "./drive/My Drive/Untitled document (7).gdoc\n",
            "./drive/My Drive/Sophocles notes.gdoc\n",
            "./drive/My Drive/Polling notes.gdoc\n",
            "./drive/My Drive/Greek Conventions.gdoc\n",
            "./drive/My Drive/database\n",
            "./drive/My Drive/database/sec 5.gdraw\n",
            "./drive/My Drive/database/sec5.1.gdraw\n",
            "./drive/My Drive/database/sec5.2.gdraw\n",
            "./drive/My Drive/database/5.3.gdraw\n",
            "./drive/My Drive/database/sec 5 .gdoc\n",
            "./drive/My Drive/database/6.4.gdraw\n",
            "./drive/My Drive/database/sec 6.gdoc\n",
            "./drive/My Drive/database/7.1.gdraw\n",
            "./drive/My Drive/database/Untitled document (1).gdoc\n",
            "./drive/My Drive/database/Untitled drawing (2).gdraw\n",
            "./drive/My Drive/database/7.2.gdraw\n",
            "./drive/My Drive/database/Untitled drawing (1).gdraw\n",
            "./drive/My Drive/database/sec 9.gdoc\n",
            "./drive/My Drive/database/9.2 #1.gdraw\n",
            "./drive/My Drive/database/9.1.gdraw\n",
            "./drive/My Drive/database/9.2 #2.gdraw\n",
            "./drive/My Drive/database/9.2.gdraw\n",
            "./drive/My Drive/database/Untitled drawing.gdraw\n",
            "./drive/My Drive/database/11.gdoc\n",
            "./drive/My Drive/database/ch 17.gdoc\n",
            "./drive/My Drive/database/semester exam.gdoc\n",
            "./drive/My Drive/database/Untitled document.gdoc\n",
            "./drive/My Drive/database/sql 1.gdoc\n",
            "./drive/My Drive/database/sql 2.gdoc\n",
            "./drive/My Drive/database/15.gdoc\n",
            "./drive/My Drive/database/chapter 16.gdoc\n",
            "./drive/My Drive/common app essays.gdoc\n",
            "./drive/My Drive/Untitled document (6).gdoc\n",
            "./drive/My Drive/Descriptive Essay.gdoc\n",
            "./drive/My Drive/Untitled document (5).gdoc\n",
            "./drive/My Drive/Poetry notes.gdoc\n",
            "./drive/My Drive/rec.gdoc\n",
            "./drive/My Drive/U of I essays.gdoc\n",
            "./drive/My Drive/commonappessaysd2 (1).docx\n",
            "./drive/My Drive/ui of commonappessaysd2.docx\n",
            "./drive/My Drive/u of i essays.gdoc\n",
            "./drive/My Drive/ui of commonappessaysd2.docx.gdoc\n",
            "./drive/My Drive/recs and stuff.gdoc\n",
            "./drive/My Drive/Lit notes.gdoc\n",
            "./drive/My Drive/judicial branch.gdoc\n",
            "./drive/My Drive/Graph theory lesson plan.gdoc\n",
            "./drive/My Drive/thesis.gdoc\n",
            "./drive/My Drive/Clark University.gdoc\n",
            "./drive/My Drive/short response .gdoc\n",
            "./drive/My Drive/commonappessaysd2.docx\n",
            "./drive/My Drive/Important Supreme Court Cases.gdoc\n",
            "./drive/My Drive/writing notes.gdoc\n",
            "./drive/My Drive/How not to procrastinate .gdoc\n",
            "./drive/My Drive/inauguration .gdoc\n",
            "./drive/My Drive/gov chapter 13.gdoc\n",
            "./drive/My Drive/Maristhigh_school.docx.gdoc\n",
            "./drive/My Drive/at.gdoc\n",
            "./drive/My Drive/Process Analysis Paper.gdoc\n",
            "./drive/My Drive/IIT.gdoc\n",
            "./drive/My Drive/astronomy.gdoc\n",
            "./drive/My Drive/writing suggestions #2.gdoc\n",
            "./drive/My Drive/sec 8.gdoc\n",
            "./drive/My Drive/mccormick qs and essay.gdoc\n",
            "./drive/My Drive/Untitled document (4).gdoc\n",
            "./drive/My Drive/comp politics\n",
            "./drive/My Drive/comp politics/soveriegnty .gdoc\n",
            "./drive/My Drive/comp politics/Untitled document (1).gdoc\n",
            "./drive/My Drive/comp politics/Untitled document.gdoc\n",
            "./drive/My Drive/cs125\n",
            "./drive/My Drive/cs125/8 30 2013.gdoc\n",
            "./drive/My Drive/chem102\n",
            "./drive/My Drive/chem102/Stoich.gdoc\n",
            "./drive/My Drive/Miracle Whip draft.gdoc\n",
            "./drive/My Drive/Rhet notes 105.gdoc\n",
            "./drive/My Drive/Untitled document (3).gdoc\n",
            "./drive/My Drive/Reading Response 9.gdoc\n",
            "./drive/My Drive/reading response 10.gdoc\n",
            "./drive/My Drive/Untitled document (2).gdoc\n",
            "./drive/My Drive/research draft 1.gdoc\n",
            "./drive/My Drive/Untitled document (1).gdoc\n",
            "./drive/My Drive/Bernauer, Andrew_school letter 7-2015-signed.pdf\n",
            "./drive/My Drive/A. bernauer letter.pdf\n",
            "./drive/My Drive/attachments\n",
            "./drive/My Drive/attachments/mime-attachment\n",
            "./drive/My Drive/Cloud EML Reader\n",
            "./drive/My Drive/Cloud EML Reader/SKMBT_C65415051215500.pdf\n",
            "./drive/My Drive/Cloud EML Reader/mime-attachment (1)\n",
            "./drive/My Drive/Cloud EML Reader/mime-attachment\n",
            "./drive/My Drive/William Carlos Williams, The Use of Force.pdf\n",
            "./drive/My Drive/The Meat.pdf\n",
            "./drive/My Drive/Barn Burning Essay (1).gdoc\n",
            "./drive/My Drive/Barn Burning Essay.gdoc\n",
            "./drive/My Drive/Geography 106 (1).gdoc\n",
            "./drive/My Drive/Midterm3ExamStudyGuide.pdf\n",
            "./drive/My Drive/Geography 106.gdoc\n",
            "./drive/My Drive/BrokeBack Mountain Essay.gdoc\n",
            "./drive/My Drive/Good Country .gdoc\n",
            "./drive/My Drive/The Emerging Arctic.pdf\n",
            "./drive/My Drive/GEOG_106_AP2_Fall2015.pdf\n",
            "./drive/My Drive/Geography 106 Paper 2.gdoc\n",
            "./drive/My Drive/GEOG_106_AP1_Fall2015.pdf\n",
            "./drive/My Drive/Geography 106 paper 1.gdoc\n",
            "./drive/My Drive/Geography 353\n",
            "./drive/My Drive/Geography 353/April 28, 2016.gdoc\n",
            "./drive/My Drive/Econ 490 response paper 1 fogel.gdoc\n",
            "./drive/My Drive/1 26 2016 Geography Notes.gdoc\n",
            "./drive/My Drive/Geography 2 18 2016.gdoc\n",
            "./drive/My Drive/Untitled document.gdoc\n",
            "./drive/My Drive/Geography 357 2 23 2016.gdoc\n",
            "./drive/My Drive/Geography notes 3 3 2016.gdoc\n",
            "./drive/My Drive/Response Paper 3.gdoc\n",
            "./drive/My Drive/Geography midterm.gdoc\n",
            "./drive/My Drive/South Asia Midterm TakeHome 2016.pdf\n",
            "./drive/My Drive/Econ 490 midterm study guide.gdoc\n",
            "./drive/My Drive/Econ 490 3 15 2016.gdoc\n",
            "./drive/My Drive/Python Portfolio.gdoc\n",
            "./drive/My Drive/april 12, 2016.gdoc\n",
            "./drive/My Drive/response paper 6.docx\n",
            "./drive/My Drive/GhertnerExercise2.doc\n",
            "./drive/My Drive/GhertnerExercise2.doc.gdoc\n",
            "./drive/My Drive/GhertnerExercise3.doc\n",
            "./drive/My Drive/GhertnerExercise3.doc.gdoc\n",
            "./drive/My Drive/Geog 356 3 15 2016.gdoc\n",
            "./drive/My Drive/FormulaSheet.pdf\n",
            "./drive/My Drive/Econ 490 chapter 2.gdoc\n",
            "./drive/My Drive/stataIII_workshop_dataset.dta\n",
            "./drive/My Drive/stataII_workshop_dataset.dta\n",
            "./drive/My Drive/stata_workshop_DOfile.do\n",
            "./drive/My Drive/Philosophy 105.gdoc\n",
            "./drive/My Drive/M6_trade.xlsx\n",
            "./drive/My Drive/M6_trade.xlsx.gsheet\n",
            "./drive/My Drive/MLB 17 Weekly Upgrade Predictions.gsheet\n",
            "./drive/My Drive/kindt 484\n",
            "./drive/My Drive/kindt 484/Drug Prohibition_ An Unnatural Disaster.pdf\n",
            "./drive/My Drive/kindt 484/Economics of Prohibition_2.pdf\n",
            "./drive/My Drive/kindt 484/miron_2.pdf\n",
            "./drive/My Drive/kindt 484/drug potency.pdf\n",
            "./drive/My Drive/kindt 484/RandyEBarnettTheHarmfulSi.pdf\n",
            "./drive/My Drive/kindt 484/DavidRHendersonAHumaneEco.pdf\n",
            "./drive/My Drive/kindt 484/NoraVDemleitnerOrganizedC.pdf\n",
            "./drive/My Drive/kindt 484/MitchellRosenthalOppositi.pdf\n",
            "./drive/My Drive/kindt 484/Citations:.gdoc\n",
            "./drive/My Drive/kindt 484/Cover Page.gdoc\n",
            "./drive/My Drive/kindt 484/an unnatural disater.gdoc\n",
            "./drive/My Drive/kindt 484/Drug Potency.gdoc\n",
            "./drive/My Drive/kindt 484/miron.gdoc\n",
            "./drive/My Drive/kindt 484/organized crime.gdoc\n",
            "./drive/My Drive/kindt 484/Henderson.gdoc\n",
            "./drive/My Drive/kindt 484/opposition.gdoc\n",
            "./drive/My Drive/kindt 484/The harm.gdoc\n",
            "./drive/My Drive/kindt 484/Resume .gdoc\n",
            "./drive/My Drive/response paper 6.docx.gdoc\n",
            "./drive/My Drive/Untitled spreadsheet (2).gsheet\n",
            "./drive/My Drive/Stat note sheet 2.gdoc\n",
            "./drive/My Drive/Mosaic complaint.docx\n",
            "./drive/My Drive/Mosaic complaint.docx.gdoc\n",
            "./drive/My Drive/Andrew Bernauer job application cover letter.docx\n",
            "./drive/My Drive/New Doc 2018-01-31.pdf\n",
            "./drive/My Drive/Untitled spreadsheet (1).gsheet\n",
            "./drive/My Drive/Chapter 14 Globalization.pptx.gslides\n",
            "./drive/My Drive/20180325_143929 (1).jpg\n",
            "./drive/My Drive/japanesemusic_1.jpg\n",
            "./drive/My Drive/japenese_music2.jpg\n",
            "./drive/My Drive/20180325_143750 (1).jpg\n",
            "./drive/My Drive/20180325_143742.jpg\n",
            "./drive/My Drive/20180325_143733 (1).jpg\n",
            "./drive/My Drive/20180325_143627.jpg\n",
            "./drive/My Drive/20180325_143719 (1).jpg\n",
            "./drive/My Drive/20180325_143609.jpg\n",
            "./drive/My Drive/20180325_143553.jpg\n",
            "./drive/My Drive/20180325_143539.jpg\n",
            "./drive/My Drive/20180325_143507.jpg\n",
            "./drive/My Drive/20180325_143455.jpg\n",
            "./drive/My Drive/20180325_143502.jpg\n",
            "./drive/My Drive/20180325_143435.jpg\n",
            "./drive/My Drive/20180325_143413.jpg\n",
            "./drive/My Drive/20180325_143804.jpg\n",
            "./drive/My Drive/20180325_143308.jpg\n",
            "./drive/My Drive/20180325_143351.jpg\n",
            "./drive/My Drive/20180325_143929.jpg\n",
            "./drive/My Drive/20180325_143750.jpg\n",
            "./drive/My Drive/20180325_143719.jpg\n",
            "./drive/My Drive/20180325_143733.jpg\n",
            "./drive/My Drive/Mus 133 Essay 3 .gdoc\n",
            "./drive/My Drive/Stat 385 proposal.gdoc\n",
            "./drive/My Drive/pithchf x.pdf\n",
            "./drive/My Drive/Music 133.gdoc\n",
            "./drive/My Drive/Geography 221.gdoc\n",
            "./drive/My Drive/Music 133 Essay 4.gdoc\n",
            "./drive/My Drive/Datalytics.gslides\n",
            "./drive/My Drive/Python\n",
            "./drive/My Drive/Python/Pythonic practice .gdoc\n",
            "./drive/My Drive/math 209 practice\n",
            "./drive/My Drive/math 209 practice/Math 209 - Practice Test 2 (Fall 2015).PDF\n",
            "./drive/My Drive/Econ 399 (1)\n",
            "./drive/My Drive/Econ 399 (1)/Econ 399 .gdoc\n",
            "./drive/My Drive/Econ 399\n",
            "./drive/My Drive/bernaue2jobdocsresume.docx\n",
            "./drive/My Drive/Red sox project application\n",
            "./drive/My Drive/Red sox project application/Sample Project Description.gdoc\n",
            "./drive/My Drive/bernaue2jobdocsresume.gdoc\n",
            "./drive/My Drive/Blue Jays Resume questions .gdoc\n",
            "./drive/My Drive/Untitled folder\n",
            "./drive/My Drive/English 261\n",
            "./drive/My Drive/English 261/Notes 12 3 2018.gdoc\n",
            "./drive/My Drive/English 261/Notes 12 10 2018.gdoc\n",
            "./drive/My Drive/English 261/Slow Violence Reflection Paper bernaue2.gdoc\n",
            "./drive/My Drive/homework4.pdf\n",
            "./drive/My Drive/homework4.gdoc\n",
            "./drive/My Drive/English 261 final paper.gdoc\n",
            "./drive/My Drive/bernaue2 homework.pdf\n",
            "./drive/My Drive/andrewbernauerresumefall2018.docx\n",
            "./drive/My Drive/andrewbernauerresumefall2018.gdoc\n",
            "./drive/My Drive/Geog 371\n",
            "./drive/My Drive/Geog 371/Notes 1 16 2018.gdoc\n",
            "./drive/My Drive/Geog 371/Assignment 1.gdoc\n",
            "./drive/My Drive/Geog 371/1 23 2018.gdoc\n",
            "./drive/My Drive/Geog 371/1\\28\\2019.gdoc\n",
            "./drive/My Drive/Geog 371/2\\4\\2019.gdoc\n",
            "./drive/My Drive/Geog 371/2\\6\\2019.gdoc\n",
            "./drive/My Drive/Geog 371/2 11 2019.gdoc\n",
            "./drive/My Drive/Geog 371/2 13 2019\n",
            "./drive/My Drive/Geog 371/2 13 2019/Notes.gdoc\n",
            "./drive/My Drive/Geog 371/2 18 2019.gdoc\n",
            "./drive/My Drive/Geog 371/2\\25\\2019.gdoc\n",
            "./drive/My Drive/Geog 371/Notes 2\\27\\2019.gdoc\n",
            "./drive/My Drive/Geog 371/3\\6\\2019\n",
            "./drive/My Drive/Geog 371/3\\6\\2019/Notes .gdoc\n",
            "./drive/My Drive/Geog 371/3 25 2018.gdoc\n",
            "./drive/My Drive/Geog 371/Notes 4\\1\\2019.gdoc\n",
            "./drive/My Drive/Geog 371/Untitled document (1).gdoc\n",
            "./drive/My Drive/Geog 371/Geog 371 4\\15\\2019.gdoc\n",
            "./drive/My Drive/Geog 371/Untitled document.gdoc\n",
            "./drive/My Drive/Geog 371/Notes 4 24 2019 .gdoc\n",
            "./drive/My Drive/CS 225\n",
            "./drive/My Drive/CS 225/Untitled document.gdoc\n",
            "./drive/My Drive/ECON 399\n",
            "./drive/My Drive/ECON 399/The myth of the rational voter.gdoc\n",
            "./drive/My Drive/ECON 399/Respone paper one .gdoc\n",
            "./drive/My Drive/ex1_bernaue2_Sp2019.gdoc\n",
            "./drive/My Drive/spanish response uno.gdoc\n",
            "./drive/My Drive/ECON 490 ML\n",
            "./drive/My Drive/ECON 490 ML/Notes.gdoc\n",
            "./drive/My Drive/ECON 490 ML/3 26.gdoc\n",
            "./drive/My Drive/andrewbernauer_resume_spring2019.gdoc\n",
            "./drive/My Drive/spanish response dos.gdoc\n",
            "./drive/My Drive/Notes .gdoc\n",
            "./drive/My Drive/MLPACK build script\n",
            "./drive/My Drive/MLPACK build script/Untitled document.gdoc\n",
            "./drive/My Drive/Econ 490 ML\n",
            "./drive/My Drive/Formula sheet.gdoc\n",
            "./drive/My Drive/C++ notes\n",
            "./drive/My Drive/C++ notes/effective modern C ++.gdoc\n",
            "./drive/My Drive/C++ notes/C++ Templates the complete guide.gdoc\n",
            "./drive/My Drive/MLPACK application\n",
            "./drive/My Drive/MLPACK application/R binding workflow.gdoc\n",
            "./drive/My Drive/Project proposal.gdoc\n",
            "./drive/My Drive/Copy of Project proposal Automatically-Generated R bindings.gdoc\n",
            "./drive/My Drive/ECON responses\n",
            "./drive/My Drive/ECON responses/Chapter 2 and 3 response.gdoc\n",
            "./drive/My Drive/GEOG Poster.gslides\n",
            "./drive/My Drive/Copy of andrewbernauer_resume_spring2019 (1).gdoc\n",
            "./drive/My Drive/datascience_resume_spring2019.gdoc\n",
            "./drive/My Drive/Rcpppmlpack bindings\n",
            "./drive/My Drive/Rcpppmlpack bindings/CMake notes.gdoc\n",
            "./drive/My Drive/Interview Prep Notes.gdoc\n",
            "./drive/My Drive/R extensions notes\n",
            "./drive/My Drive/R extensions notes/Notes on Extensions Manual .gdoc\n",
            "./drive/My Drive/Untitled spreadsheet.gform\n",
            "./drive/My Drive/Untitled spreadsheet.gsheet\n",
            "./drive/My Drive/R consortium \n",
            "./drive/My Drive/Job search\n",
            "./drive/My Drive/Job search/Karat job question.gdoc\n",
            "./drive/My Drive/Job search/Untitled spreadsheet.gsheet\n",
            "./drive/My Drive/Job search/Resume stuff.gdoc\n",
            "./drive/My Drive/Job search/Second Petal.gdoc\n",
            "./drive/My Drive/Job search/First Petal Worksheet.gdoc\n",
            "./drive/My Drive/Job search/IProspect preparation.gdoc\n",
            "./drive/My Drive/Job search/Home Advisors Internship Interview Prep.gdoc\n",
            "./drive/My Drive/Job search/Resume Notes.gdoc\n",
            "./drive/My Drive/Cover Letters\n",
            "./drive/My Drive/Cover Letters/RockAuto Cover Letter.gdoc\n",
            "./drive/My Drive/Sample Growth Plan.gdoc\n",
            "./drive/My Drive/Cover Letter Rock Auto.gdoc\n",
            "./drive/My Drive/Copy of andrewbernauer_resume_spring2019.gdoc\n",
            "./drive/My Drive/Econ 490 Response Paper 2.gdoc\n",
            "./drive/My Drive/Rcppmlpack2.gdoc\n",
            "./drive/My Drive/Copy of andrewbernauer_resume_spring2019 (4).pdf\n",
            "./drive/My Drive/SC Johnson\n",
            "./drive/My Drive/SC Johnson/Introduction.gdoc\n",
            "./drive/My Drive/Time log.gsheet\n",
            "./drive/My Drive/ANDREW BERNAUR RESUME REVIEW (1).docx\n",
            "./drive/My Drive/ANDREW BERNAUR RESUME REVIEW.docx\n",
            "./drive/My Drive/Project proposal Automatically-Generated R bindings.gdoc\n",
            "./drive/My Drive/ANDREW BERNAUR RESUME REVIEW (1).gdoc\n",
            "./drive/My Drive/ANDREW BERNAUR RESUME REVIEW.gdoc\n",
            "./drive/My Drive/ Resume abern SP2020.gdoc\n",
            "./drive/My Drive/ML abern SP2020.gdoc\n",
            "./drive/My Drive/Colab Notebooks\n",
            "./drive/My Drive/Colab Notebooks/Copy of Chapter2:building blocks of neural nets.ipynb\n",
            "./drive/My Drive/Colab Notebooks/Chapter2:building blocks of neural nets.ipynb\n",
            "./drive/My Drive/Colab Notebooks/ListComphrensions.ipynb\n",
            "./drive/My Drive/Colab Notebooks/Chapter3_Getting_started_with_neural_networks.ipynb\n",
            "./drive/My Drive/Colab Notebooks/Chapter_4Fundamentalsofmachinelearning.ipynb\n",
            "./drive/My Drive/Colab Notebooks/dogs-vs-cats.zip\n",
            "./drive/My Drive/Colab Notebooks/Chapter5Deeplearningforcomputervision.ipynb\n",
            "./drive/.Trash\n",
            "./sample_data\n",
            "./sample_data/anscombe.json\n",
            "./sample_data/README.md\n",
            "./sample_data/california_housing_train.csv\n",
            "./sample_data/mnist_train_small.csv\n",
            "./sample_data/mnist_test.csv\n",
            "./sample_data/california_housing_test.csv\n",
            "find: ‘*.zip’: No such file or directory\n",
            "Archive:  ./drive/My Drive/Colab Notebooks/dogs-vs-cats.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of ./drive/My Drive/Colab Notebooks/dogs-vs-cats.zip or\n",
            "        ./drive/My Drive/Colab Notebooks/dogs-vs-cats.zip.zip, and cannot find ./drive/My Drive/Colab Notebooks/dogs-vs-cats.zip.ZIP, period.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0OSZvD2CriY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "173f4281-6023-4b35-dec4-2307d32896ca"
      },
      "source": [
        "import os, shutil\n",
        "# /content/drive/My Drive/\n",
        "# print(os.getcwd())\n",
        "# current wd is gdrive\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AT6CKPxbtmD",
        "colab_type": "text"
      },
      "source": [
        "Psuedo code to refactor "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqehLmkCbuMd",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "import os, shutil\n",
        "\n",
        "orinal_dataset_dir = '/Users/fchollet/Downloads/kaggle_original_data'\n",
        "\n",
        "base_dir = '/Users/fchollet/Downloads/cats_and_dogs_small'\n",
        "os.mkdir(base_dir)\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir, 'test')\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.mkdir(test_dir)\n",
        "\n",
        "train_cats\n",
        "\n",
        "```\n"
      ]
    }
  ]
}