{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter2:building blocks of neural nets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMDAm7uTR3ozYsbYc0Mu7GZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abernauer/Deep-Learning-with-Python/blob/master/Chapter2_building_blocks_of_neural_nets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15Xao3sy4U-p",
        "colab_type": "text"
      },
      "source": [
        "# 2.1 A first look at a neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i9a3o0O1-uu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "f02c60e1-b7dd-4ea1-d47a-beb381a21d61"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(train_images, train_lables), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOKcK99J57DV",
        "colab_type": "text"
      },
      "source": [
        " *Training data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1NsdYLz5qQm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6e6209da-af1a-469b-a3a8-b113527d54d1"
      },
      "source": [
        "train_images.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvAOAcQi6V1B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3610280f-b7b2-4191-efac-d9cf728591ce"
      },
      "source": [
        "len(train_lables)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Onr6794F6c1R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d47d6ec7-f57e-48be-e324-43acd412c615"
      },
      "source": [
        "train_lables"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vkXSA06-z-",
        "colab_type": "text"
      },
      "source": [
        "*Testing data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Wbh510k7G9V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "30a44d57-01cb-4447-dbaf-2c2ce714f652"
      },
      "source": [
        "test_images.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV-uwW607OhD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2ce84b85-42fc-45bc-85c6-fcca10c0bfb9"
      },
      "source": [
        "len(test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEyz51hO7S8P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ed4e3871-e046-4d11-ac62-d514a48a0a74"
      },
      "source": [
        "test_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No924VNq8BfA",
        "colab_type": "text"
      },
      "source": [
        "*Network Architechture*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mrt8zpW8NfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMz6uV6L8lam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = models.Sequential()\n",
        "net.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "net.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtVzp-YkGD97",
        "colab_type": "text"
      },
      "source": [
        "*compilation step*\n",
        "\n",
        "* A loss function how we measure our performance on the training data\n",
        "* An optimizer the mechanism in which, the network will update based on the data and loss function\n",
        "* Metrics to monitor during testing and training in this case accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J0Uyra69waq",
        "colab_type": "text"
      },
      "source": [
        "*Compilation Step*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je2i0hZI9qqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net.compile(optimizer='rmsprop',\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvuq-qQk-Z52",
        "colab_type": "text"
      },
      "source": [
        "*Preparing the data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nws3lMO-dt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeIUktuMAc74",
        "colab_type": "text"
      },
      "source": [
        "*Preparing the labels*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewzqmG1GAk-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "train_lables = to_categorical(train_lables)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzFFbkPqBIPF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "bb0c6664-4e41-41e2-830f-2a83269e4e42"
      },
      "source": [
        "net.fit(train_images, train_lables, epochs=5, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.2561 - accuracy: 0.9268\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.1039 - accuracy: 0.9698\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.0680 - accuracy: 0.9793\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.0501 - accuracy: 0.9851\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.0378 - accuracy: 0.9890\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f773f1da7b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpcy-ArIFEyo",
        "colab_type": "text"
      },
      "source": [
        "The quantities displayed above are the loss and accuracy of the training. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okKpzjNVFXaC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7511e4a5-1b13-41ea-cea0-e8ac6bfc70e8"
      },
      "source": [
        "test_loss, test_acc = net.evaluate(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 49us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMOMjBuSFqSF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f89c6474-7fb1-460b-be53-a5e11598b9f0"
      },
      "source": [
        "print(\"test_acc:\", test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_acc: 0.9811999797821045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdGdlv9hPGI5",
        "colab_type": "text"
      },
      "source": [
        "#2.2 Data representations for neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB-8ltxoPUVC",
        "colab_type": "text"
      },
      "source": [
        "The data in the previous example was stored in a multidimensional Numpy array or *tensors*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9PSIUmnPrLq",
        "colab_type": "text"
      },
      "source": [
        "A simple explanation of a tensor is a container for data--primarily numeric data. In the context of tensors a *dimension* is often called an *axis*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcmeMVKMQ4-p",
        "colab_type": "text"
      },
      "source": [
        "# 2.2.1 Scalars (0D tensors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jst8IE1dRqP0",
        "colab_type": "text"
      },
      "source": [
        "A 0D tensor containing a only one number is called a *scalar*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z7Aaml4UGHF",
        "colab_type": "text"
      },
      "source": [
        "In Numpy, a float32 or float64 number is a scalar tensor. The number of axes of a tensor is called it's *rank*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v-Vqb2STp_d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b5928e21-57b7-47b9-bdc2-8bba29fbdb58"
      },
      "source": [
        "import numpy as np\n",
        "x = np.array(12)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rqzHeyGT-xr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3f329aee-9583-48c7-eb5c-315f05bf514a"
      },
      "source": [
        "x.ndim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-wcb0b_UrVl",
        "colab_type": "text"
      },
      "source": [
        "#2.2.2 Vectors (1D tensors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMp_uGGPVONk",
        "colab_type": "text"
      },
      "source": [
        "*Vector* or 1D tensor is an array of numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qANUK_vxQFZu",
        "colab_type": "text"
      },
      "source": [
        "A 1D tensor has exactly one axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhSn4F82VM8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.array([12, 3, 6, 14, 7])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2eEGRxIQv5F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2bc59a8d-4503-4ab0-ab2e-9bfd4062d859"
      },
      "source": [
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([12,  3,  6, 14,  7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWRRQ3CCQz81",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "de50df49-8d8c-48c8-b966-2f470865e61d"
      },
      "source": [
        "x.ndim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X-4R-XeRFgl",
        "colab_type": "text"
      },
      "source": [
        "A vector with five entries or elements is a *5-dimensional vector*.\n",
        "*Dimensionality* can denote the number of entries or elements along a specific axis or the number of axis in a tensor. So a 5D tensor would be a *tensor of rank 5*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgjovIP6SDER",
        "colab_type": "text"
      },
      "source": [
        "#2.2.3 Matrices (2D tensors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyNqB71iSKeB",
        "colab_type": "text"
      },
      "source": [
        "An array of vectors is a *matrix*, or 2D tensor. A matrix has two axes referred to as *rows* and *columns*.\n",
        "A visual interpretation of a matrix as a rectangular grid of numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30GTlxolTcqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.array( [[5, 78, 2, 34, 0],\n",
        "             [6, 79, 3, 35, 1],\n",
        "             [7, 80, 4, 36, 2]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U98au2vOUCQB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e331fb38-2765-4795-f300-74cf1fda7299"
      },
      "source": [
        "x.ndim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAYLnLETUOwh",
        "colab_type": "text"
      },
      "source": [
        "The entries from the first axis are called the *rows*, and the elements from the second axis are called the *columns*. [5, 78, 2, 34, 0] is the first row of x and [5, 6, 7] is the first column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE_PrPy0WxVz",
        "colab_type": "text"
      },
      "source": [
        "#2.2.4 3D tensors and higher-dimensional tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sunhon09X0K9",
        "colab_type": "text"
      },
      "source": [
        "If we pack a matrix in a new array, you obtain a 3D tensor, which you can visually interpret as a cube of numbers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up-7WXBbYOur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.array([[[5, 78, 2, 34, 0],\n",
        "                   [6, 79, 3, 35, 1],\n",
        "                   [7, 80, 4, 36, 2]],\n",
        "                  [[5, 78, 2, 34, 0],\n",
        "                   [6, 79, 3, 35, 1],\n",
        "                   [7, 80, 4, 36, 2]],\n",
        "                  [[5, 78, 2, 34, 0],\n",
        "                   [6, 79, 3, 35, 1],\n",
        "                   [7, 80, 4, 36, 2]]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC1ViipB9kT9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "efe8adae-108e-4145-b8ec-f0900ad41964"
      },
      "source": [
        "x.ndim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU3N3vc69pKv",
        "colab_type": "text"
      },
      "source": [
        "Packing 3D tensors in an arrray, you create a 4D tensor. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8FXnXvm99AM",
        "colab_type": "text"
      },
      "source": [
        "#2.2.5 Key attributes "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4co6ho6-ECi",
        "colab_type": "text"
      },
      "source": [
        "A tensor is defined by three key attributes:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWEiqTZd_66A",
        "colab_type": "text"
      },
      "source": [
        "* *Number of axes (rank)*--For instance, a 3D tensor has three axes, and a matrix has two axes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO2N5ozWk8Z7",
        "colab_type": "text"
      },
      "source": [
        "* *Shape*-- This is a tuple of intergers that describes how many dimensions the tensor has along each axis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdIentbxlsWH",
        "colab_type": "text"
      },
      "source": [
        "* *Data type* (called dtype in Python libraries)--This is the type of the data contained in the tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNuGi6b5qKsB",
        "colab_type": "text"
      },
      "source": [
        "To drive home the point on data we can examine the mnist data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-okA3lHq_1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from  keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l0ZaZH9tIHE",
        "colab_type": "text"
      },
      "source": [
        "We display the number of axes of the tensor train_images\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_vQ1lgnbpP1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "79d3ba90-b036-45ec-cb9d-dd3e9bf95d3e"
      },
      "source": [
        "print(train_images.ndim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0OJix9qc6EW",
        "colab_type": "text"
      },
      "source": [
        "The shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI5VSa5Ic5W1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "86639732-9b9b-4a30-f229-6b1dd996ab11"
      },
      "source": [
        "print(train_images.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZKyq2Mhez0s",
        "colab_type": "text"
      },
      "source": [
        "And the tensor's data type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi8TLPkSe4or",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "59563297-94ed-436f-c2c4-f7db63a9e243"
      },
      "source": [
        "print(train_images.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "uint8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Gw9_bhKm9A5",
        "colab_type": "text"
      },
      "source": [
        "So our data is a 3D tensor of 8-bit integers. Or an array of 60,000 matrices of 28 x 28 integers. Each matrix being greyscale, with coefficients between 0 and 255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyqV-RKUn3JC",
        "colab_type": "text"
      },
      "source": [
        "We can display the fourth digit with matplot lib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYNyOH9doeEF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "bb0b8efb-5dca-4239-b78a-454df7da4a4a"
      },
      "source": [
        "digit = train_images[4]\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(digit, cmap=plt.cm.binary)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DDSK3pwplbf",
        "colab_type": "text"
      },
      "source": [
        "#2.2.6 Manipulating tensors in Numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKTTUg2UqmG6",
        "colab_type": "text"
      },
      "source": [
        "Selecting specific elements in a tensor is called *tensor slicing* . It was performed with train_imgages[i] syntax."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfzsLkOqrHbn",
        "colab_type": "text"
      },
      "source": [
        "The following example selecs digits #10 to #100 (#100 isn't included) and puts them in array of shape (90, 28, 28):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZVXUjWdrGi5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "869ab455-60b9-41b6-ce46-afeacaddb698"
      },
      "source": [
        "my_slice = train_images[10:100, :, :]\n",
        "my_slice.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8Z4pmeHuaAQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bd876f1d-c5fc-48b0-dfd1-4e0635e5165f"
      },
      "source": [
        "my_slice = train_images[10:100, 0:28, 0:28]\n",
        "my_slice.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vekWm0mdu5lC",
        "colab_type": "text"
      },
      "source": [
        "In general, you may select between any two indices along each tensor axis.  For instance in order to select 14 X 14 pixels centered in the middle, you do this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqzkLb2XvXLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_slice = train_images[:, 14:, 14:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnLPYl_Rvul1",
        "colab_type": "text"
      },
      "source": [
        "It's also possible to use negative indices. To crop patches of pixels out of an image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ncj-Z8Bwd9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_slice = train_images[:, 7:-7, 7:-7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsdGUzXIxwcR",
        "colab_type": "text"
      },
      "source": [
        "# 2.2.7 The notion of data batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDAGOi_cx4ar",
        "colab_type": "text"
      },
      "source": [
        "In general, the first axis(axis 0, because indexing starts at 0) in all data tensor you'll come across in deep learning will be the *samples axis* (sometimes called the *samples dimension*). In the MNIST example samples are images of digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIQ65p_Ty1_V",
        "colab_type": "text"
      },
      "source": [
        "In addition, deep-learning models don't process an entire dataset at once; rather, they break the data into small batches. Concretely, here's one batch of our MNIST digits with the batch size of 128:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbmRsPblzy7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch = train_images[:128]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lWn5rokz7Db",
        "colab_type": "text"
      },
      "source": [
        "And the next batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqBSsqzbz_Yi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch = train_images[128:256]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuJz6RqM0Jz2",
        "colab_type": "text"
      },
      "source": [
        "And the nth batch:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxq-hgRW0l6v",
        "colab_type": "text"
      },
      "source": [
        "batch = train_images[128 * n:128 * (n + 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7aaec2j1DaF",
        "colab_type": "text"
      },
      "source": [
        "When considering such a batch tensor, the first axis (axis 0) is called the *batch axis* or *batch dimension*. This is a term you'll frequently encounter when using Keras and other deep-learning libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0maBDLIC1fCq",
        "colab_type": "text"
      },
      "source": [
        "#2.2.8 Real-world examples of data tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWflOzP82Djt",
        "colab_type": "text"
      },
      "source": [
        "* *Vector data* -- 2D tensors of shape (samples, features)\n",
        "* *Timeseries data or sequence data* -- 3D tensors of shape (samples, timesteps, features)\n",
        "* *Images* -- 4D tensors of shape (samples, frames, height, width, channels) or (samples, frames, channels, height, width)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVnLXIWp3KX0",
        "colab_type": "text"
      },
      "source": [
        "#2.2.9 Vector data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cYyp4oy3o77",
        "colab_type": "text"
      },
      "source": [
        "This is the most common case. In such a dataset, each single data point can be encoded as a vector, and thus a batch of data will be encoded as a 2D tensor, where the first axis is the *samples axis* and the second axes is the *feature axis*.\n",
        "Two examples:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1bhke_A5Rjp",
        "colab_type": "text"
      },
      "source": [
        "* An actuarial dataset of people, where we consider each person's age, ZIP code, and income. Each person can be characterized as a vector of 3 values, and thus an entire dataset of 100,000 people can be stored in a 2D tensor of shape (1000000, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jllcZDc96Q5g",
        "colab_type": "text"
      },
      "source": [
        "* A dataset of text documents, where we represent each document by the counts of how many times each word appears in it ( out of a dictionary of 20,000 words). Each document can be encoded as a vector of 20,000 values (one count per word in the dictionary), and thus an entire dataset of 500 documents can be stored in a tensor of shape (500, 20000)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGAJFMtN8Jfp",
        "colab_type": "text"
      },
      "source": [
        "#2.2.10 Timeseries data or sequence data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCHML3BD8VH-",
        "colab_type": "text"
      },
      "source": [
        "Whenever time matters in your data (or the notion of sequence order), it makes sense to store it in a 3D tensor with an explicit time axis. Each sample can be encoded as a sequence of vector (a 2D tensor), and thus a batch of data will be encoded as a 3D tensor. The time is always the second axis (axis of index 1) Let's look at some examples:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBVA2Dz79-Mg",
        "colab_type": "text"
      },
      "source": [
        "* A dataset of stock prices. Every minute, we store the current price of the stock, the highest price in the past minute, and the lowest price in the past minute. Thus every minute is encoded as a 3D vector, an entire day of trading is encoded as a 2D tensor of shape (390, 3) (there are 390 minutes in a trading day), and 250 days' worth of data can be stored in a 3D tensor of shape (250, 390, 3). Here, each sample would be one day's worth of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zlgf7Z6_Vvi",
        "colab_type": "text"
      },
      "source": [
        "* A dataset of tweets, where we encode tweet as a sequence of 280 characters out of an alphabet of 128 unique characters. In this setting, each character can be encoded as a binary vector of size 128( an all-zeros vector except for a 1 entry at the index corresponding to the character). Then each tweet can be encoded as a 2D tensor of shape (280, 128), and a dataset of 1 million twwets can be stored in a tensor of shape (1000000, 280, 128)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FOob0bbAujd",
        "colab_type": "text"
      },
      "source": [
        "#2.2.11 Image data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVva9p-JA4YP",
        "colab_type": "text"
      },
      "source": [
        "Images typically have three dimensions: heigth, width, and color depth. Although grayscale images (like our MNIST digits) have only a single color channel and could thus be stored in 2D tensors, by convention image tensors are always 3D, with a one-dimensional color channel for grayscale images. A batch of 128 grayscale images of size 256 x 256 could thus be stored in a tensor of shape (128, 256, 256, 1), and a batch of 128 color images could be stored in a tensor could be stored in a tensor of shape (128, 256, 256, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqnCDYssDuiD",
        "colab_type": "text"
      },
      "source": [
        "There are two conventions for shapes of images tensors: the *channel--last* convention (used by TensorFlow) and the *channel-first* convention (used by Theano). The TensorFlow machine-learning framework from Google places the color-depth axis at the end: (samples, height, width, color_depth). Meanwhile, Theano places the color depth axis right after the batch axis: (samples, color_depth, height, width). With the Theano convention, the previous examples would become (128, 1, 256, 256)\n",
        "and (128, 3, 256, 256). The keras framework provides support for both formats."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOmWadmtFJm6",
        "colab_type": "text"
      },
      "source": [
        "#2.2.12 Video data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC6h8FiHMOeu",
        "colab_type": "text"
      },
      "source": [
        "#2.3 The gears of neural networks: tensor operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvIPebQANDTT",
        "colab_type": "text"
      },
      "source": [
        "Much as any computer program can be ultimately reduced to a small set of binary operations on binary inputs (AND, OR, NOR, and so on), all transformations learned by deep neural networks can be reduced to a handful of *tensor operations* applied to tensors of numeric data. For instance, it's possible to add tensors, multiply tensors, and so on. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpyx4qV-N2O6",
        "colab_type": "text"
      },
      "source": [
        "In our intial example, we were building our network by stacking Dense layers on top of each other. A Keras layer instance looks like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2IzHUT1Oyy-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "```\n",
        "keras.layers.Dense(512, activation='relu')\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS0sO5OEPN5F",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "This layer can be interpreted as a function, which takes as input a 2D tensor and returns another 2D tensor-- a new representation for the input tensor. Specifically, the function is as follows (where W is a 2D tensor and b is a vector, both attributes of the layer):\n",
        "```\n",
        "output = relu(dot(W, input) + b)\n",
        "```\n",
        "We have three tensor operations here: a dot product(*dot*) between the input tensor and a tensor named *W*; an addittion (+) between the resulting 2D tensor and a vector *b*; and, finally, a *relu* operation relu(x) is max(x, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kgmk4QHWWMQH",
        "colab_type": "text"
      },
      "source": [
        "#2.3.1 Element-wise operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBsjCysdX1X4",
        "colab_type": "text"
      },
      "source": [
        "The *relu* operation and addition are *element-wise* operations: operations that are applied independently to each entry in the tesnors being considered. This means these operations are highly amenable to massivly parrallel implementations(*vectorized* implementations, a term that comes from vector processor supercomputer architecture)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oC23c6qWzaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def naive_relu(x):\n",
        "    assert len(x.shape) == 2\n",
        "\n",
        "    x = x.copy()\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] = max(x[i, j], 0)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKip6g-fWSyo",
        "colab_type": "text"
      },
      "source": [
        "The same for addition:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmXR-g7JrIz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def naive_add(x, y):\n",
        "    assert len(x.shape) == 2\n",
        "    assert x.shape == y.shape\n",
        "\n",
        "    x = x.copy()\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] += y[i, j]\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYVzwwmXsLUM",
        "colab_type": "text"
      },
      "source": [
        "So, in Numpy, you can do the following element-wise operations, and it will be blasing fast:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFvPogCMswPq",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "import numpy as np\n",
        "\n",
        "z = x + y\n",
        "\n",
        "z = np.maximum(z, 0.)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esbVjKiaItoc",
        "colab_type": "text"
      },
      "source": [
        "# 2.3.2 Broadcasting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wadlWy7xIzeb",
        "colab_type": "text"
      },
      "source": [
        "The naive implementation of *naive_add* only supports the addition of 2D tensors with identical shapes. However the *Dense* layer, we added a 2D tensor with a vector. What happens when the shapes of the tensors added differ?\n",
        "\n",
        "When possible, the smaller tensor will be *broadcasted* to match the shape of the larger tensor. Broadcasting consists of two steps:\n",
        "\n",
        "1. Axes (called *broadcast axes*) are add to the smaller tensor to match the *ndim* of the largest tensor.\n",
        "\n",
        "2. The samller tensor is repeated alongside these new azes to match the full shape of the larger tensor.\n",
        "\n",
        "Consider X with shape (32, 10) and y with shape (10,). First we add an empty axis to y, whose shape become (1,10). Then, we repeat y 32 times alongside this new axis, so that we end up with a tensor of Y with shape (32, 10), where Y[i, :]==y for i in range(0, 32). At this point, we can proceed to ad X and Y, because they have the same shape. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9hgycW0O8ew",
        "colab_type": "text"
      },
      "source": [
        "In terms of implementation, no new 2D tensor is created, because that would be terribly inefficient. But think of the vector being repeted 10 times alongside a new axis is a helpful mental model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHQl7lGgQGH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def naive_add_matrix_and_vector(x, y):\n",
        "    assert len(x.shape) == 2\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "\n",
        "    x = x.copy()\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] += y[j]\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFNReP25RLo9",
        "colab_type": "text"
      },
      "source": [
        "With broadcasting, you can generally apply two-tensor element-wise operations if one tensor has shape (a, b, ... n, n+1, ...m) and the other shape (n. n+1, ...m). The broadcasting will then automatically happen for axes a through n -1. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMCUMJpIR98B",
        "colab_type": "text"
      },
      "source": [
        "Example of performing an element-wise *maximum* operation to two tensors of different shapes via brodcasting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grHXRU5vSR79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.random.random((64, 3, 32, 10))\n",
        "y = np.random.random((32, 10))\n",
        "\n",
        "z = np.maximum(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HaQRgurEpU4",
        "colab_type": "text"
      },
      "source": [
        "#2.3.3 Tensor dot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omkIEgLbTdV7",
        "colab_type": "text"
      },
      "source": [
        "The dot operation, also called a *tensor product* (not to be confused with an element wise product). Contrary to element-wise operations, it combines entries in the input tensors. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h2fzL4kUI3r",
        "colab_type": "text"
      },
      "source": [
        "An element wise product is perfomed with the * operator in Numpy, Keras, Theano, and TensorFlow. *dot* uses a different syntax in TensorFlow, but in both Numpy and Keras it's done using the standard *dot operator:\n",
        "```python\n",
        "import numpy as np\n",
        "z = np.dot(x,y)\n",
        "```\n",
        "In math notation, you would use (.):\n",
        "```python\n",
        "z = x .y\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U5U2dXkVlo6",
        "colab_type": "text"
      },
      "source": [
        "A naive implementation of the dot product implemented below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acE03mxNEov7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def naive_vector_dot(x, y):\n",
        "    assert len(x.shape) == 1\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[0] == y.shape[0]\n",
        "    z = 0\n",
        "    for i in range(x.shape[0]):\n",
        "        z += x[i] * y[i]\n",
        "    return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdwi6jxjWfCq",
        "colab_type": "text"
      },
      "source": [
        "Notice the dot product between two vectors is a scalar and that only vectors with the same number of elements are compatible for a dot prouct. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDBBmaKwXH9r",
        "colab_type": "text"
      },
      "source": [
        "You can also take a dot product between a matrix x and vector y, which returns a vector where the coefficients are dot products between y and the rows of x. :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_B7ojQHXcZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "def naive_matrix_vector_dot(x, y):\n",
        "    assert len(x.shape) == 2 \n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "\n",
        "    z = np.zeroes(x.shape[0])\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            z[i] += x[i, j] * y[j]\n",
        "    return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hksziipgY33K",
        "colab_type": "text"
      },
      "source": [
        "An alternative approache could reuse the the code from the vector product:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rca9GocEZL3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def naive_matrix_vector_dot(x, y):\n",
        "    z = np.zeroes(x.shape[0])\n",
        "    for i in range(x.shape[0]):\n",
        "        z[i] = naive_vector_dot(x[i, :], y)\n",
        "    return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyNyOVRXZs7Y",
        "colab_type": "text"
      },
      "source": [
        "Note that as soon as one of the two sensors has an *ndim* greater than 1, *dot* is no longer symmetric, which is to say dot(x, y) isn't the same as dot(y, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn2AC66jn3x7",
        "colab_type": "text"
      },
      "source": [
        "Of course, a dot product generalizes to tensors with an arbritary number of axes. The most common applications may be the dot product between two matrices. \n",
        "Here's the naive implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMjbUf7wppMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def naive_matrix_dot(x, y):\n",
        "    assert len(x.shape) == 2\n",
        "    assert len(y.shape) == 2\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "\n",
        "    z = np.zeros((x.shape[0]), y.shape[1])\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(y.shape[1]):\n",
        "            row_x = x[i, :]\n",
        "            column_y = y[:, j]\n",
        "            z[i, j] = naive_vector_dot(row_x, column_y)\n",
        "    return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qznr45L1rG0W",
        "colab_type": "text"
      },
      "source": [
        "To understand dot-product\n",
        "to do get photo of matrix multiplication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8w6sZ5DEx9w",
        "colab_type": "text"
      },
      "source": [
        "#2.3.4 Tensor reshaping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JEBsHUk304Y",
        "colab_type": "text"
      },
      "source": [
        "A third type of tensor operation that's essential to understand is *tensor reshaping*. Which was used when preproccesed the digits data. :\n",
        "```\n",
        "trian_images = trains_images.reshape((60000, 28 * 28)\n",
        "\n",
        "```\n",
        "Reshaping a tensor means rearranging its rows and columns to match a target shape. Examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdWub4_-42hD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "x = np.array([[0., 1.],\n",
        "              [2., 3.],\n",
        "              [4., 5.]])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3Z39mWZ5VDc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8aaaf64e-4707-4ec0-b86a-befb909e7627"
      },
      "source": [
        "print(x.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwZKXbD25cDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = x.reshape((6, 1))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdCF2e015jMU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "384e5a72-4d06-4e06-c7ea-ef5832a05cfb"
      },
      "source": [
        "x"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.],\n",
              "       [1.],\n",
              "       [2.],\n",
              "       [3.],\n",
              "       [4.],\n",
              "       [5.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu9-6L7L5uKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = x.reshape((2, 3))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YvSqnNp534I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d29fc9e3-57b6-4647-f80f-286e9f91797e"
      },
      "source": [
        "x"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 2.],\n",
              "       [3., 4., 5.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N7ezzMA6J0y",
        "colab_type": "text"
      },
      "source": [
        "A special case of reshaping that's commonly encountered is *transposition*. *Transposing* a matrix means exchanging its rows and its columns, so that x[i, :] becomes [:, i]:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Rnh0g1O6zu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.zeros((300, 20))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zgOYkq56-7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.transpose(x)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkAqihm07Eng",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2a71ff51-8689-4be1-a949-a8b66b2b2e86"
      },
      "source": [
        "print(x.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qka5r9mfE68B",
        "colab_type": "text"
      },
      "source": [
        "#2.3.5 Geometric interpretation of tensor operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr7Ph-oF7qLv",
        "colab_type": "text"
      },
      "source": [
        "Because the contents of the tensors manipulated by tensor operations can be interpreted as coordinates of points in some geometric space, all tensor operations have a geometric interpretation. First example addittion. \n",
        "\n",
        "```\n",
        "A = [0.5, 1]\n",
        "\n",
        "```\n",
        "It's a point in a 2D space.\n",
        "\n",
        "revisit "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxuc18nn85qB",
        "colab_type": "text"
      },
      "source": [
        " * add figures\n",
        " * vector addittion etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb-I9qLwFE9-",
        "colab_type": "text"
      },
      "source": [
        "#2.3.6 A geometric interpretation of deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovxIvBgY829h",
        "colab_type": "text"
      },
      "source": [
        "You just learned that nueral networks consist entirely of chains of tensor operation and that all of these tensor operations are just gemetric transformations of the input data. One could distill a neural network down as a very complex geometric transformation in a high-dimensional space, a long a series of simple steps. \n",
        "\n",
        "#2.4 The engine of nueral networks:gradient-based optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WykzRupF-jQI",
        "colab_type": "text"
      },
      "source": [
        "As you saw in the previous section, each neural layer from our first network example transforms its input data as follows:\n",
        "````\n",
        "output = relu(dot(W, input) + b)\n",
        "````\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CatM81f-7un",
        "colab_type": "text"
      },
      "source": [
        "In the above expression, *W* and *b* are tensors that are attributs of the layer. They're called the *weights* or *trainable parameters* of the layer ( the *kernel* and *bias* attributes, respectively). These weights contain the information learned by the network from exposure to training data. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYHRwlDUAIcN",
        "colab_type": "text"
      },
      "source": [
        "Initially, these weight matrices are filled with small random values( a step called *random initialization*) Of course, there's no reason to expect that relu(dot(W, input) + b), when *W* and *b* are randwom, will yield any useful representations. The resulting representations are meaningless--but they're a starting point. What comes next is to gradually adjust these weights, based on a feedback signal. The gradual adjustment, also called *training*, is basically the learning that machine learning is all about. \n",
        "\n",
        "This happens within within what's called a *training loop*, which works as follows. Repeat these steps in a loop, as long as necessary:\n",
        "\n",
        "1. Draw a batch of training samples x and corresponding targets y. \n",
        "\n",
        "2. Run the network on x(a step called the *forward pass*) to obtain predictions y_pred.\n",
        "\n",
        "3. Compute the loss of the network on the batch, a measure of mismatch between y_pred and y.\n",
        "\n",
        "4. Update all weights of the network in a way that slightly reduces the loss on this batch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "651Pp_rFCkEo",
        "colab_type": "text"
      },
      "source": [
        "You'll eventually end up with a network that has a very low loss on it's trianing data: a low mismatch between predictions y_pred and expected targets y. The network has \"learned\" to map its inputs to correct to correct targets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSMVytqDEebt",
        "colab_type": "text"
      },
      "source": [
        "#2.4.1 What's a derivative?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeOy8U9gEvft",
        "colab_type": "text"
      },
      "source": [
        "Consider a continuous, smooth function f(x) = y, mapping a real number x to a new real number y. Because the function is *continuous*, a small change in x can only result in a small change in y-- that's the intuition behind continuity. Let's say you increase x by a small factor epsilon_x: this results in a small epsilon_y change to y:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxNH_dlZFdm4",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "f(x + epsilon_x) = y + epsilon_y\n",
        "```\n",
        "In addittion, because the function is *smooth* , when epsilon_x is small enough, around a certain point *p*. it's possible to approximate *f* as a linear function of slope *a*, so that epsilon_y becomes a * epsilon_x:\n",
        "\n",
        "```\n",
        "f(x + epsilon_x) = y + a * epsilon_x\n",
        "```\n",
        "Obviously, the linear approximation is valid only when x is close enough to *p*\n",
        "\n",
        "The slope a is called the *derivative* of f in p. If a is negative, it means a small change of x around p will result in a decrease of f(x); and if a is positive, a small chang in x will result in an increase of f(x)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GayPq-j9I2dT",
        "colab_type": "text"
      },
      "source": [
        "#2.4.2 Derivative of a tensor operation: the gradient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZJ6Z1wyJB_Z",
        "colab_type": "text"
      },
      "source": [
        "A *gradient* is the derivative of a tensor operation. It's the generalization of derivatives to functions of multidimensional inputs: that is, to functions that take tensors as inputs. \n",
        "\n",
        "Consider an input vector *x*, a matrix *W*, a target *y*, and a loss function *loss*. You can use *W* to compute a target candidate y_pred, and comput the loss, or mismatch, between the target candidate y_pred and the target y:\n",
        "\n",
        "```\n",
        "y_pred = dot(W, x)\n",
        "loss_value = loss(y_pred, y)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he_p0W6ZKMvO",
        "colab_type": "text"
      },
      "source": [
        "If the data inputs x and y are frozen, then this can be interpreted as a function mapping values of *W* to loss values:\n",
        "```\n",
        "loss_value = f(W)\n",
        "```\n",
        "Let's say the current value of *W* is *W0*. Then the derivative of f in the point W0 is a tensor gradient(f)(W0) with the same shape as *W*, where each coefficient gradient(f)(wO)[i,j] indicates the direction and magnitude of the change in *loss_value* you observe whe modifying W0[i, j]. That tensor gradient (f)(wO) is the gradient of the function f(W) = loss_value in Wo.\n",
        "\n",
        "You saw earlier that the derivative of a function f(x) of a single coefficient can be interpreted as the slope of the curve of f. Likewise, gradient(f)(WO) can be interpreted as the tensor describing the curvature of f(W) around W0.\n",
        "\n",
        "For this reason, in much the same way that, for a function f(x), you can reduce the value of f(x) by moving x a little in the opposite direction from the derivative, with a function f(w) of a tensor, you can reduce f(W) by moving *W* in the opposite direction from the gradient: for example, W1 = W0 - step * gradient(f)(w0) (where step is a small scaling factor). That means going against the curvature, which intuitively should put you lower on the curve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97Sjv0jXNw9o",
        "colab_type": "text"
      },
      "source": [
        "#2.4.3 Stochastic gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_90lJyeAN7mm",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}