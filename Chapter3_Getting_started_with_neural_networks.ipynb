{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter3_Getting_started_with_neural_networks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO32hjmZzNjtGkYbf1j3mKn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abernauer/Deep-Learning-with-Python/blob/master/Chapter3_Getting_started_with_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfx-vE8r0zt2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#Chapter 3 Getting started with neural networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hQXwxmv1ZYQ",
        "colab_type": "text"
      },
      "source": [
        "#3.1 Anatomy of a neural networks "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EWOy38I1jnc",
        "colab_type": "text"
      },
      "source": [
        "Training a neural network revolves around the following objects:\n",
        "\n",
        "* Layers which are combined into a network (or model)\n",
        "* The input data and corresponding targets\n",
        "* The loss function, which defines the feedback signal used for learning.\n",
        "* The optimizer, which determines how learning proceeds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5LwzShc2U69",
        "colab_type": "text"
      },
      "source": [
        "Add diagram of the components and how they interact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWlHQitz2fyY",
        "colab_type": "text"
      },
      "source": [
        "#3.1.1 Layers the building blocks of deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDpqPFk62mhe",
        "colab_type": "text"
      },
      "source": [
        "The fundamental data structure in neural networks is the *layer*, to which you were introduced in chapter 2. A layer is a data-processing module that takes as input one or more tensors and that outputs one or more tensors. Some layers are stateless but more frequently layers have a state: the layer's *weights*, one or several tensors learned with stochastic gradient descent, which together contain the network's *knowledge*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytLYJo0ytNiW",
        "colab_type": "text"
      },
      "source": [
        "Different layers are appropriate for different tensor formats and different types of data processing. For instance, simple vector data, stored in 2D tensors of shape (samples, features), is often processed by *densely connected* layers, also called *fully connected* or *dense* layers. Sequence data, stored in 3D tensors of shape (samples, timesteps, features), is typically processed by *recurrent* layers such as an *LSTM* layer.\n",
        "Image data, stored in 4D tensor, is usually processed by 2D convolution layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAC1oivFu2Ia",
        "colab_type": "text"
      },
      "source": [
        "A metaphor for layers is thinking of them as LEGO bricks for deep learning. Building deep-learning models in Keras is done by clipping together compatible layers to form useful data-transformation pipelines. The notion of layer *compatibility* here refers specifically to the fact that every layer will only accept input tensor of a certain shape and will return output tensors of a certain shape. Example: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nmdRQQAwA-b",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "from keras import layers\n",
        "\n",
        "layer = layers.Dense(32, input_shape=(784,))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4CmDBXfwWyD",
        "colab_type": "text"
      },
      "source": [
        "We're creating a layer that will only accept as input 2D tensors where the first dimension is 784(axis 0, the batch dimension, is unspecified, and thus any value would be accepted.) This layer will return a tensor where the first dimension has been transformed to be 32."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5jLWc8Dw5vI",
        "colab_type": "text"
      },
      "source": [
        "Thus this layer can only be connected to a downstream layer that expects 32-dimensional vectors as its input. When using Keras, you don't have to worry about compatibility, because the layers you add to your models are dynamically built to match the shape of the incoming layer. For instance, suppose you write the following: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYwyIxulyVha",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(32, input_shape=(784,)))\n",
        "model.add(layers.Dense(32))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHYc_iOUy-7y",
        "colab_type": "text"
      },
      "source": [
        "The second layer will automatically infer its input shape as being the the output shape of the layer that came before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOTWzMAuzTcJ",
        "colab_type": "text"
      },
      "source": [
        "#3.1.2 Models: networks of layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zceheV1d0Dq2",
        "colab_type": "text"
      },
      "source": [
        "A deep-learning model is a directed, acyclic graph of layers. The most common instance is a linear stack of layers, mapping a single input to a single output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek8rClDZ0c2b",
        "colab_type": "text"
      },
      "source": [
        "Some broader networks include:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJGWYNEX0nc9",
        "colab_type": "text"
      },
      "source": [
        "* Two-branch networks\n",
        "* Multihead networks\n",
        "* Inception blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA5nhR9A0y_Y",
        "colab_type": "text"
      },
      "source": [
        "The topology of a network defines a *hypothesis space*. You may remember that in chapter 1, we defined machine learning as \"searching for useful representations of some input data, within a predefined space of possibilities, using guidance from a feedback signal\". By choosing a network topology, you constrain your *space of possibilities* (hypothesis space) to a specific series of tensor operations, mapping input data to output data. What you'll then be searching for is a good set of values for the weight tensors involved in these tensor operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-u7zffa28Pb",
        "colab_type": "text"
      },
      "source": [
        "#3.1.3 Loss functions and optimizers: keys to configuring the learning process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIH40nrY3H4J",
        "colab_type": "text"
      },
      "source": [
        "Once the network architecture is defined, you still have to choose two more things:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOWZEQ763R0L",
        "colab_type": "text"
      },
      "source": [
        "* Loss function (objective function)-- The quantity that will be minimized during training. It represents a measure of success for the task at hand.\n",
        "* Optimizer--Determines how the network will be updated based on the loss function. It implements a specific variant of stochastic gradient descent(SGD)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEW1zQ3n31ev",
        "colab_type": "text"
      },
      "source": [
        "Choosing the right objective function for the right problem is extremely important: your network will take any shortcut it can, to minimize the loss; so if the objective doesn't fully correlate with success for the task at hand your network will end up doing things you may not have wanted. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1UDh-VV5h0b",
        "colab_type": "text"
      },
      "source": [
        "Guidelines for chosing the correct loss:\n",
        "* binary crossentropy\n",
        "  - two-class classification problem\n",
        "* categorical crossentropy\n",
        "  - many-class classification problem\n",
        "* mean squared error\n",
        "  - regression problems\n",
        "* Connectionist temporal classification\n",
        "  - sequence-learning problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAmTKUn49V_5",
        "colab_type": "text"
      },
      "source": [
        "#3.2.2 Developing with Keras: a quick overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OswH9C7CAR9m",
        "colab_type": "text"
      },
      "source": [
        "The typical Keras workflow looks like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3hWIO5uAct9",
        "colab_type": "text"
      },
      "source": [
        "1. Define your training data: input tensors and target tensors.\n",
        "2. Define a network of layers (or *model*) that maps your inputs to your targets.\n",
        "3. Configure the learning process by choosing a loss function, an optimizer, and some metrics to monitor.\n",
        "4. Iterate on your training data by calling the fit() method of your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HPmQvfRBF3M",
        "colab_type": "text"
      },
      "source": [
        "There are two ways to define a model: using the *Sequential* class(only for linear stacks of layers, which is the most common architecture) or the *functional* API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS5dXKiEEA4e",
        "colab_type": "text"
      },
      "source": [
        "As a refresher, here's a two-layer model defined using the *Sequential* class (note that we're passing the expected shape of the input data to the first layer):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxiMq3HUEY5X",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(32, activation='relu', input_shape=(784,)))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAvkUJj7E-2S",
        "colab_type": "text"
      },
      "source": [
        "And the same model using the functional API:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoOpBpnrFEVI",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "input_tensor = layers.Input(shape=(784,))\n",
        "x = layers.Dense(32, activation='relu')(input_tensor)\n",
        "\n",
        "output_tensor = layers.Dense(10, activation='softmax') (x)\n",
        "model = models.Model(inputs=input_tensor, outputs=output_tensor)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_yYOouBGr0F",
        "colab_type": "text"
      },
      "source": [
        "With the functional API, you're maniplating the data tensors that the model proccesses and applying layers to this tensor as if they were functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKGXgbs2HHcs",
        "colab_type": "text"
      },
      "source": [
        "Once your model architectre is defined, it doesn't matter whether you used a *Sequential* model or the functional API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKSsV5S3HV-f",
        "colab_type": "text"
      },
      "source": [
        "The learning process is configured in the compilation step, where you specify the optimizer and loss function(s) that the model should use, as well as the metrics to monitor during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTSF3nTgHtnX",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "from keras import optimizers\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
        "loss='mse',\n",
        "metrics=['accuracy'])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eF_CTn_ISX8",
        "colab_type": "text"
      },
      "source": [
        "Finally, the learning process consists of passing Numpy arrays of input data ( and the corresponding target data) to the model via the fit() method, similar to what you would do in SciKit-Learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP5qEdG4IupH",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "model.fit(input_tensor, target_tensor, batch_size=128, epochs=10)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1rb1cc7MsTJ",
        "colab_type": "text"
      },
      "source": [
        "#3.4 Classifying movie reviews: a binary classification example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32MgITYLNB1L",
        "colab_type": "text"
      },
      "source": [
        "Two-class classification, or binary classifictaion, may be the most widely applied kind of machine-learning problem. In this example, you'll learn to classify movie reviews as positive or negative, based on the text contents of the reviews.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2bTt0znRZx5",
        "colab_type": "text"
      },
      "source": [
        "#3.4.1 The IMDB dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pny_vNOFRfKu",
        "colab_type": "text"
      },
      "source": [
        "You'll work with th IMDB dataset: as set of 50,000 highly polarized reviews from the Internet Movie Database. They're split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPlfB_6wUHWN",
        "colab_type": "text"
      },
      "source": [
        "Why use separate training and test sets? Because you should never test a machine learning model on the same data you used to train it! Just because a model performs well on its training data doesn't mean it will perform well on data it has never seen; and what you care about is your model's performance on new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98-T_OwtVD2V",
        "colab_type": "text"
      },
      "source": [
        "The following code will load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33ngwhDoNF4a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "9fe5fce8-c024-4c4a-dea5-d965f8c0bc42"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "(train_data, train_labels),  (test_data, test_labels) = imdb.load_data(num_words=10000)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Mt8LIwAWF4f",
        "colab_type": "text"
      },
      "source": [
        "The argument num_words=10000 means you'll only keep the top 10,000 most frequently occurring words in the training data. Rare words will be discarded. This allows you to work with vector data of manageable size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6VJ4Fq8XNwT",
        "colab_type": "text"
      },
      "source": [
        "The variables train_data and test_data are lists of reviews; each review is a list of word indices(encoding a sequence of words). train_labels and test_labels are lists of 0s and 1s, where 0 stands for *negative* and 1 stands for *positive*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7buJtq6Nmf9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a7edec92-1b3f-46a6-bfb4-5618af71caec"
      },
      "source": [
        "train_data[0]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 2,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 2,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 2,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 2,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 2,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 2,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULKLwbx1Nw18",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "05cb94af-d652-4468-c421-581d4784b998"
      },
      "source": [
        "train_labels[0]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T48W4vZkN1v7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f19d34d3-a24d-4445-eb9d-a110d3d08c46"
      },
      "source": [
        "max([max(sequence) for sequence in train_data])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKiXR2NuOcxQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "3a4a080b-3a9b-4665-ed30-d2cbfbd3736b"
      },
      "source": [
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "decoded_review = ' '.join(\n",
        "    [reverse_word_index.get(i - 3, '?') for i in train_data[0]]\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wAv6SW0fShE",
        "colab_type": "text"
      },
      "source": [
        "#3.4.2 Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C1Ia3gofVx3",
        "colab_type": "text"
      },
      "source": [
        "You can't feed lists of integers into a neural networks. You have to turn your lists into tensors. There are two ways to do that:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8kbHXcbfiYG",
        "colab_type": "text"
      },
      "source": [
        "* Pad your lists so that they all have the same length, turn them into an integer tensor of shape (samples, word_indices), and then use as the first layer in your network a layer capable of handling such integer tensors ( the Embedding layer, which we'll cover in detail later in the book).\n",
        "\n",
        "* One-hot encode your lists to turn them into vectors of 0s and 1s. This would mean, for instance, turning the sequence [3, 5] into a 10,000-dimensional vector that would be all 0s except for indices 3 and 5, which would be 1s. Then you could use as the first layer in your network a *Dense* layer, capable of handling floating-point vector data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rvsM0jYhqVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1\n",
        "    return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lfybgc35jJSu",
        "colab_type": "text"
      },
      "source": [
        "What the samples look like now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fhTr1WzjNWy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9ebcfa3d-193d-4ced-c568-6aed765dd629"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 1., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a3eflPkjSx5",
        "colab_type": "text"
      },
      "source": [
        "We should also vectorize the lables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORTgIZiQjZjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = np.array(train_labels).astype('float32')\n",
        "y_test = np.array(test_labels).astype('float32')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVSYy74hjtRh",
        "colab_type": "text"
      },
      "source": [
        "No we can build our network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q32mNdGDjxyA",
        "colab_type": "text"
      },
      "source": [
        "#3.4.3 Building your network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8tw4lm8l6Kw",
        "colab_type": "text"
      },
      "source": [
        "The input data is vectors, and the labels are scalars(1s and 0s): this is the easiest setup you'll ever encounter. A type of network that performs well on such a problem is a simple stack of fully connected (Dense) layers with *relu* activations: Dense(16,activation='relu').\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oigAEFNimek8",
        "colab_type": "text"
      },
      "source": [
        "The argument being passed to each Dense layer (16) is the number of hidden units of the layer. A *hidden unit* is a dimension in the representation space of the layer. You may remember from chapter 2 that each *Dense* layer with a relu activation implements the following chain of tensor operations:\n",
        "```\n",
        "output = relu(dot(W, input) + b)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0MZxHIonRSI",
        "colab_type": "text"
      },
      "source": [
        "Having 16 hidden units means the weight matrix *W* will have shape(input_dimension, 16): the dot product with *W* will project the input data onto a 16-dimensional representation space. You can intuitively understand the dimensionality of your representation space as \"how much freedom you're allowing the network to have when learning the internal representations.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "282Wy18toNp6",
        "colab_type": "text"
      },
      "source": [
        "There are two key architecture decisions to be made about such a stack of *Dense* layers:\n",
        "\n",
        "* How many layers to use\n",
        "* How many hidden units to choose for each layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgQ8IL9VolSp",
        "colab_type": "text"
      },
      "source": [
        "More on this in chapter 4 we will us the following architecture:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIDYowA1otfg",
        "colab_type": "text"
      },
      "source": [
        "* Two intermediate layers with 16 hidden units each\n",
        "* A third layer that will output the scalar prediction regarding the sentiment of the current review."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHmu1WToqwiu",
        "colab_type": "text"
      },
      "source": [
        "The intermediate layers will use *relu* as their activation function, and the final layer will use a sigmoid activation so as to output a probability (a score between 0 and 1, indicating how likely the sample is to have the target \"1\": how likely the review is to be positive). A *relu* (rectified linear unit) is a function meant ot zero out negative values whereas a sigmoid \"squashes\" arbitrary values into the [0, 1] interval, outputting something that can be interpreted as a probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El2_IwpPr0-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74k3nlv2stwa",
        "colab_type": "text"
      },
      "source": [
        "It is key to you use an activation function as it allows for non-linear transformations of the hypothesis space. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gh3BvGNs8bU",
        "colab_type": "text"
      },
      "source": [
        "Finally, we choose a loss function and an optimizer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAAr7iLntIts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80B8uQQzvRlb",
        "colab_type": "text"
      },
      "source": [
        "#3.4.4 Validating your approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP0Z1g4DvrYr",
        "colab_type": "text"
      },
      "source": [
        "In order to monitor during training the accuracy of the model on data it has never seen before, you'll create a validation set by setting apart 10,000 samples from the orginal training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl2mZLGKwBBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgtY6x91wj9o",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHMJtmMDwlxZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "outputId": "2ca57e82-6efb-4c2e-e1bb-6d6043d18fba"
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "15000/15000 [==============================] - 3s 186us/step - loss: 0.5118 - acc: 0.7874 - val_loss: 0.3816 - val_acc: 0.8730\n",
            "Epoch 2/20\n",
            "15000/15000 [==============================] - 2s 154us/step - loss: 0.3022 - acc: 0.9049 - val_loss: 0.3087 - val_acc: 0.8829\n",
            "Epoch 3/20\n",
            "15000/15000 [==============================] - 2s 154us/step - loss: 0.2210 - acc: 0.9287 - val_loss: 0.2990 - val_acc: 0.8791\n",
            "Epoch 4/20\n",
            "15000/15000 [==============================] - 2s 152us/step - loss: 0.1730 - acc: 0.9439 - val_loss: 0.2749 - val_acc: 0.8890\n",
            "Epoch 5/20\n",
            "15000/15000 [==============================] - 2s 154us/step - loss: 0.1433 - acc: 0.9537 - val_loss: 0.2811 - val_acc: 0.8855\n",
            "Epoch 6/20\n",
            "15000/15000 [==============================] - 2s 149us/step - loss: 0.1140 - acc: 0.9658 - val_loss: 0.3252 - val_acc: 0.8755\n",
            "Epoch 7/20\n",
            "15000/15000 [==============================] - 2s 149us/step - loss: 0.0994 - acc: 0.9705 - val_loss: 0.3091 - val_acc: 0.8845\n",
            "Epoch 8/20\n",
            "15000/15000 [==============================] - 2s 148us/step - loss: 0.0804 - acc: 0.9774 - val_loss: 0.3275 - val_acc: 0.8820\n",
            "Epoch 9/20\n",
            "15000/15000 [==============================] - 2s 149us/step - loss: 0.0690 - acc: 0.9816 - val_loss: 0.3603 - val_acc: 0.8759\n",
            "Epoch 10/20\n",
            "15000/15000 [==============================] - 2s 148us/step - loss: 0.0542 - acc: 0.9873 - val_loss: 0.3843 - val_acc: 0.8729\n",
            "Epoch 11/20\n",
            "15000/15000 [==============================] - 2s 145us/step - loss: 0.0460 - acc: 0.9905 - val_loss: 0.4096 - val_acc: 0.8767\n",
            "Epoch 12/20\n",
            "15000/15000 [==============================] - 2s 148us/step - loss: 0.0374 - acc: 0.9912 - val_loss: 0.4371 - val_acc: 0.8758\n",
            "Epoch 13/20\n",
            "15000/15000 [==============================] - 2s 148us/step - loss: 0.0315 - acc: 0.9930 - val_loss: 0.4626 - val_acc: 0.8731\n",
            "Epoch 14/20\n",
            "15000/15000 [==============================] - 2s 146us/step - loss: 0.0224 - acc: 0.9971 - val_loss: 0.4942 - val_acc: 0.8721\n",
            "Epoch 15/20\n",
            "15000/15000 [==============================] - 2s 145us/step - loss: 0.0189 - acc: 0.9973 - val_loss: 0.5268 - val_acc: 0.8679\n",
            "Epoch 16/20\n",
            "15000/15000 [==============================] - 2s 142us/step - loss: 0.0170 - acc: 0.9969 - val_loss: 0.5598 - val_acc: 0.8667\n",
            "Epoch 17/20\n",
            "15000/15000 [==============================] - 2s 143us/step - loss: 0.0116 - acc: 0.9989 - val_loss: 0.5946 - val_acc: 0.8675\n",
            "Epoch 18/20\n",
            "15000/15000 [==============================] - 2s 143us/step - loss: 0.0090 - acc: 0.9994 - val_loss: 0.6555 - val_acc: 0.8588\n",
            "Epoch 19/20\n",
            "15000/15000 [==============================] - 2s 143us/step - loss: 0.0086 - acc: 0.9989 - val_loss: 0.6642 - val_acc: 0.8633\n",
            "Epoch 20/20\n",
            "15000/15000 [==============================] - 2s 143us/step - loss: 0.0046 - acc: 0.9998 - val_loss: 0.8014 - val_acc: 0.8493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EVDxQplxz41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNjVs-Nhx-C5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b5b6fb1c-e8c3-42b0-8153-80e1f366a558"
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_N2-ydXySOX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "c49dd7c0-e485-442d-aa58-c1a7b52012e8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}