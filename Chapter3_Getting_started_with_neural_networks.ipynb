{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter3_Getting_started_with_neural_networks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPuROGuBV7bo+8OYfGu9qlo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abernauer/Deep-Learning-with-Python/blob/master/Chapter3_Getting_started_with_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfx-vE8r0zt2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#Chapter 3 Getting started with neural networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hQXwxmv1ZYQ",
        "colab_type": "text"
      },
      "source": [
        "#3.1 Anatomy of a neural networks "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EWOy38I1jnc",
        "colab_type": "text"
      },
      "source": [
        "Training a neural network revolves around the following objects:\n",
        "\n",
        "* Layers which are combined into a network (or model)\n",
        "* The input data and corresponding targets\n",
        "* The loss function, which defines the feedback signal used for learning.\n",
        "* The optimizer, which determines how learning proceeds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5LwzShc2U69",
        "colab_type": "text"
      },
      "source": [
        "Add diagram of the components and how they interact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWlHQitz2fyY",
        "colab_type": "text"
      },
      "source": [
        "#3.1.1 Layers the building blocks of deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDpqPFk62mhe",
        "colab_type": "text"
      },
      "source": [
        "The fundamental data structure in neural networks is the *layer*, to which you were introduced in chapter 2. A layer is a data-processing module that takes as input one or more tensors and that outputs one or more tensors. Some layers are stateless but more frequently layers have a state: the layer's *weights*, one or several tensors learned with stochastic gradient descent, which together contain the network's *knowledge*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytLYJo0ytNiW",
        "colab_type": "text"
      },
      "source": [
        "Different layers are appropriate for different tensor formats and different types of data processing. For instance, simple vector data, stored in 2D tensors of shape (samples, features), is often processed by *densely connected* layers, also called *fully connected* or *dense* layers. Sequence data, stored in 3D tensors of shape (samples, timesteps, features), is typically processed by *recurrent* layers such as an *LSTM* layer.\n",
        "Image data, stored in 4D tensor, is usually processed by 2D convolution layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAC1oivFu2Ia",
        "colab_type": "text"
      },
      "source": [
        "A metaphor for layers is thinking of them as LEGO bricks for deep learning. Building deep-learning models in Keras is done by clipping together compatible layers to form useful data-transformation pipelines. The notion of layer *compatibility* here refers specifically to the fact that every layer will only accept input tensor of a certain shape and will return output tensors of a certain shape. Example: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nmdRQQAwA-b",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "from keras import layers\n",
        "\n",
        "layer = layers.Dense(32, input_shape=(784,))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4CmDBXfwWyD",
        "colab_type": "text"
      },
      "source": [
        "We're creating a layer that will only accept as input 2D tensors where the first dimension is 784(axis 0, the batch dimension, is unspecified, and thus any value would be accepted.) This layer will return a tensor where the first dimension has been transformed to be 32."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5jLWc8Dw5vI",
        "colab_type": "text"
      },
      "source": [
        "Thus this layer can only be connected to a downstream layer that expects 32-dimensional vectors as its input. When using Keras, you don't have to worry about compatibility, because the layers you add to your models are dynamically built to match the shape of the incoming layer. For instance, suppose you write the following: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYwyIxulyVha",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(32, input_shape=(784,)))\n",
        "model.add(layers.Dense(32))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHYc_iOUy-7y",
        "colab_type": "text"
      },
      "source": [
        "The second layer will automatically infer its input shape as being the the output shape of the layer that came before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOTWzMAuzTcJ",
        "colab_type": "text"
      },
      "source": [
        "#3.1.2 Models: networks of layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zceheV1d0Dq2",
        "colab_type": "text"
      },
      "source": [
        "A deep-learning model is a directed, acyclic graph of layers. The most common instance is a linear stack of layers, mapping a single input to a single output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek8rClDZ0c2b",
        "colab_type": "text"
      },
      "source": [
        "Some broader networks include:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJGWYNEX0nc9",
        "colab_type": "text"
      },
      "source": [
        "* Two-branch networks\n",
        "* Multihead networks\n",
        "* Inception blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA5nhR9A0y_Y",
        "colab_type": "text"
      },
      "source": [
        "The topology of a network defines a *hypothesis space*. You may remember that in chapter 1, we defined machine learning as \"searching for useful representations of some input data, within a predefined space of possibilities, using guidance from a feedback signal\". By choosing a network topology, you constrain your *space of possibilities* (hypothesis space) to a specific series of tensor operations, mapping input data to output data. What you'll then be searching for is a good set of values for the weight tensors involved in these tensor operations."
      ]
    }
  ]
}